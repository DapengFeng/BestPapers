<!-- src: https://www.computer.org/press-room/cvpr-2024-announces-best-paper-award-winners -->
<div class="uk-width-1 uk-width-3-4@l uk-margin-large-bottom">
<div class="uk-container uk-margin-medium-top">
<div class="uk-width-1">
<div class="pageHeaderBreadcrumbs">
</div>
</div>
<div class="uk-text-bold">
<h1 style="font-size: 42px; line-height: 1.2;">CVPR 2024 Announces Best Paper Award Winners</h1>
<div style="font-size: 16px; color: #333333; font-weight: 500; margin: 10px 0 30px 4px;">
IEEE Computer Society (CS) Recognizes TCPAMI Award Recipients 
</div>
</div>
</div>
<div class="uk-container">
<div class="uk-text-bold uk-text-muted uk-margin-small-top uk-margin-small-bottom">
<div style="color: #333333; font-size: 18px; font-weight: 500;"></div>
</div>
<div class="uk-text-bold uk-text-muted uk-margin-small-top uk-margin-small-bottom uk-clearfix" style="padding-bottom: 20px !important;">
<div class="uk-float-left" style="color: #333333; font-size: 16px; font-weight: 500;">
Published 06/19/2024
</div>
<div class="uk-float-right" style="color: #333333;">
Share this on:
<a href="https://www.facebook.com/sharer/sharer.php?u=https://www.computer.org/press-room/scientists-and-engineers-rank-2023-technology-trend-predictions" target="_blank" data-feathr-click-track="true" class="socialIcon777 uk-margin-small-right uk-margin-small-left" title="facebook link" data-feathr-link-aids="5cdda43ba3a493000bf82f7f">
<i class="fa fa-facebook" aria-hidden="true"></i>
</a>
<a href="https://twitter.com/intent/tweet?url=https://www.computer.org/press-room/scientists-and-engineers-rank-2023-technology-trend-predictions&amp;text=" target="_blank" data-feathr-click-track="true" class="socialIcon777 uk-margin-small-right" title="twitter link" data-feathr-link-aids="5cdda43ba3a493000bf82f7f">
<i class="fa fa-brands fa-x-twitter" aria-hidden="true"></i>
</a>
<a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://www.computer.org/press-room/scientists-and-engineers-rank-2023-technology-trend-predictions" target="_blank" data-feathr-click-track="true" class="socialIcon777 uk-margin-small-right" title="linked in link" data-feathr-link-aids="5cdda43ba3a493000bf82f7f">
<i class="fa fa-linkedin" aria-hidden="true"></i>
</a>
<a href="mailto:info@example.com?&amp;subject=&amp;cc=&amp;bcc=&amp;body=https://www.computer.org/press-room/scientists-and-engineers-rank-2023-technology-trend-predictions%0ACVPR 2024 Announces Best Paper Award Winners" target="_blank" data-feathr-click-track="true" class="socialIcon777 uk-margin-small-right" title="email link" data-feathr-link-aids="5cdda43ba3a493000bf82f7f">
<i class="fa fa-envelope" aria-hidden="true"></i>
</a>
</div>
</div>
</div>


<div class="uk-container">


<div style="font-family: Open Sans, sans-serif; position: relative; overflow: hidden;">
<p style="color: #454545; font-size: 18px; font-family: Open Sans; font-weight: 400; line-height: 1.7em;"><img fetchpriority="high" decoding="async" class="img-responsive wp-image-421557 size-full alignright" src="https://ieeecs-media.computer.org/wp-media/2024/05/16112304/CVPR24-300x300-press-release-1x.jpg" alt="" width="300" height="300" srcset="https://ieeecs-media.computer.org/wp-media/2024/05/16112304/CVPR24-300x300-press-release-1x.jpg 300w, https://ieeecs-media.computer.org/wp-media/2024/05/16112304/CVPR24-300x300-press-release-1x-150x150.jpg 150w, https://ieeecs-media.computer.org/wp-media/2024/05/16112304/CVPR24-300x300-press-release-1x-100x100.jpg 100w" sizes="(max-width: 300px) 100vw, 300px"><strong>SEATTLE, 19 June 2024</strong> – Today, during the 2024 Computer Vision and Pattern Recognition (CVPR) Conference opening session, the CVPR Awards Committee announced the winners of its prestigious Best Paper Awards, which annually recognize top research in computer vision, artificial intelligence (AI), machine learning (ML), augmented, virtual and mixed reality (AR/VR/MR), deep learning, and much more.</p>
<p style="color: #454545; font-size: 18px; font-family: Open Sans; font-weight: 400; line-height: 1.7em;">This year, from more than 11,500 paper submissions, the CVPR 2024 Awards Committee selected the following 10 winners for the honor of Best Papers during the Awards Program at CVPR 2024, taking place now through 21 June at the Seattle Convention Center in Seattle, Wash., U.S.A.</p>
<p style="color: #454545; font-size: 18px; font-family: Open Sans; font-weight: 400; line-height: 1.7em;"><strong><em>Best Papers</em></strong></p>
<ul style="color: #454545; font-size: 18px; font-family: Open Sans; font-weight: 400; line-height: 1.7em;">
<li>“<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Generative_Image_Dynamics_CVPR_2024_paper.pdf" target="_blank" rel="noopener" data-feathr-click-track="true" data-feathr-link-aids="5cdda43ba3a493000bf82f7f">Generative Image Dynamics</a>”<br>
Authors: Zhengqi Li, Richard Tucker, Noah Snavely, Aleksander Holynski<br>
The paper presents a new approach for modeling natural oscillation dynamics from a single still picture. This approach produces photo-realistic animations from a single picture and significantly outperforms prior baselines. It also demonstrates potential to enable several downstream applications such as creating seamlessly looping or interactive image dynamics.</li>
<li>“<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liang_Rich_Human_Feedback_for_Text-to-Image_Generation_CVPR_2024_paper.pdf" target="_blank" rel="noopener" data-feathr-click-track="true" data-feathr-link-aids="5cdda43ba3a493000bf82f7f">Rich Human Feedback for Text-to-Image Generation</a>”<br>
Authors: Youwei Liang, Junfeng He, Gang Li, Peizhao Li, Arseniy Klimovskiy, Nicholas Carolan, Jiao Sun, Jordi Pont-Tuset, Sarah Young, Feng Yang, Junjie Ke, Krishnamurthy Dj Dvijotham, Katherine M. Collins, Yiwen Luo, Yang Li, Kai J. Kohlhoff, Deepak Ramachandran, and Vidhya Navalpakkam<br>
This paper highlights the first rich human feedback dataset for image generation. Authors designed and trained a multimodal Transformer to predict the rich human feedback and demonstrated some instances to improve image generation.</li>
</ul>
<p style="color: #454545; font-size: 18px; font-family: Open Sans; font-weight: 400; line-height: 1.7em;">Honorable mention papers included, “<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yu_EventPS_Real-Time_Photometric_Stereo_Using_an_Event_Camera_CVPR_2024_paper.pdf" target="_blank" rel="noopener" data-feathr-click-track="true" data-feathr-link-aids="5cdda43ba3a493000bf82f7f">EventPS: Real-Time Photometric Stereo Using an Event Camera</a>” and “<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Charatan_pixelSplat_3D_Gaussian_Splats_from_Image_Pairs_for_Scalable_Generalizable_CVPR_2024_paper.pdf" target="_blank" rel="noopener" data-feathr-click-track="true" data-feathr-link-aids="5cdda43ba3a493000bf82f7f">pixelSplat: 3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction.</a>”</p>
<p style="color: #454545; font-size: 18px; font-family: Open Sans; font-weight: 400; line-height: 1.7em;"><em><strong>Best Student Papers</strong></em></p>
<ul style="color: #454545; font-size: 18px; font-family: Open Sans; font-weight: 400; line-height: 1.7em;">
<li style="list-style-type: none;">
<ul style="color: #454545; font-size: 18px; font-family: Open Sans; font-weight: 400; line-height: 1.7em;">
<li>“<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yu_Mip-Splatting_Alias-free_3D_Gaussian_Splatting_CVPR_2024_paper.pdf" target="_blank" rel="noopener" data-feathr-click-track="true" data-feathr-link-aids="5cdda43ba3a493000bf82f7f">Mip-Splatting: Alias-free 3D Gaussian Splatting</a>”<br>
Authors: Zehao Yu, Anpei Chen, Binbin Huang, Torsten Sattler, Andreas Geiger<br>
This paper introduces Mip-Splatting, a technique improving 3D Gaussian Splatting (3DGS) with a 3D smoothing filter and a 2D Mip filter for alias-free rendering at any scale. This approach significantly outperforms state-of-the-art methods in out-of-distribution scenarios, when testing at sampling rates different from training, resulting in better generalization to out-of-distribution camera poses and zoom factors.</li>
<li>“<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Stevens_BioCLIP_A_Vision_Foundation_Model_for_the_Tree_of_Life_CVPR_2024_paper.pdf" target="_blank" rel="noopener" data-feathr-click-track="true" data-feathr-link-aids="5cdda43ba3a493000bf82f7f">BioCLIP: A Vision Foundation Model for the Tree of Life</a>”<br>
Authors: Samuel Stevens, Jiaman Wu, Matthew J. Thompson, Elizabeth G. Campolongo, Chan Hee Song, David Edward Carlyn, Li Dong, Wasila M. Dahdul, Charles Stewart, Tanya Berger-Wolf, Wei-Lun Chao, and Yu Su<br>
This paper offers TREEOFLIFE-10M and BIOCLIP, a large-scale diverse biology image dataset and a foundation model for the tree of life, respectively. This work shows BIOCLIP is a strong fine-grained classifier for biology in both zero- and few-shot settings.</li>
</ul>
</li>
</ul>
<p style="color: #454545; font-size: 18px; font-family: Open Sans; font-weight: 400; line-height: 1.7em;">There also were four honorable mentions in this category this year: “<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Roetzer_SpiderMatch_3D_Shape_Matching_with_Global_Optimality_and_Geometric_Consistency_CVPR_2024_paper.pdf" target="_blank" rel="noopener" data-feathr-click-track="true" data-feathr-link-aids="5cdda43ba3a493000bf82f7f">SpiderMatch: 3D Shape Matching with Global Optimality and Geometric Consistency</a>”; “<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Miller_Objects_as_Volumes_A_Stochastic_Geometry_View_of_Opaque_Solids_CVPR_2024_paper.pdf" target="_blank" rel="noopener" data-feathr-click-track="true" data-feathr-link-aids="5cdda43ba3a493000bf82f7f">Image Processing GNN: Breaking Rigidity in Super-Resolution; Objects as Volumes: A Stochastic Geometry View of Opaque Solids</a>;” and “<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Jiang_Comparing_the_Decision-Making_Mechanisms_by_Transformers_and_CNNs_via_Explanation_CVPR_2024_paper.pdf" target="_blank" rel="noopener" data-feathr-click-track="true" data-feathr-link-aids="5cdda43ba3a493000bf82f7f">Comparing the Decision-Making Mechanisms by Transformers and CNNs via Explanation Methods.</a>”</p>
<p style="color: #454545; font-size: 18px; font-family: Open Sans; font-weight: 400; line-height: 1.7em;">“We are honored to recognize the CVPR 2024 Best Paper Awards winners,” said David Crandall, Professor of Computer Science at Indiana University, Bloomington, Ind., U.S.A., and CVPR 2024 Program Co-Chair. “The 10 papers selected this year – double the number awarded in 2023 – are a testament to the continued growth of CVPR and the field, and to all of the advances that await.”</p>
<p style="color: #454545; font-size: 18px; font-family: Open Sans; font-weight: 400; line-height: 1.7em;">Additionally, the IEEE Computer Society (CS), a CVPR organizing sponsor, announced the Technical Community on Pattern Analysis and Machine Intelligence (TCPAMI) Awards at this year’s conference. The following were recognized for their achievements:</p>
<ul style="color: #454545; font-size: 18px; font-family: Open Sans; font-weight: 400; line-height: 1.7em;">
<li><b>Longuet-Higgins Prize – </b>Awarded to a paper that has withstood the test of time, the 2024 Longuet-Higgins Prize recognizes the CVPR paper from 2014 with the most impact.
<ul style="color: #454545; font-size: 18px; font-family: Open Sans; font-weight: 400; line-height: 1.7em;">
<li><strong>2024 Recipient</strong>: “<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf" target="_blank" rel="noopener" data-feathr-click-track="true" data-feathr-link-aids="5cdda43ba3a493000bf82f7f">Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation</a>”<br>
Authors: Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik</li>
</ul>
</li>
<li><b>Young Researcher Award</b> – This award recognizes one or two researchers within seven years of receiving their Ph.D. who have made distinguished research contributions to computer vision.
<ul style="color: #454545; font-size: 18px; font-family: Open Sans; font-weight: 400; line-height: 1.7em;">
<li><strong>2024 Recipient</strong>: Angjoo Kanazawa, Carl Vondrick</li>
</ul>
</li>
<li><b>Thomas Huang Memorial Prize</b> – Established in 2020, in honor of Thomas S. Huang, one of the foremost figures in computer vision, pattern recognition and human computer interaction, of his time, the prize recognizes and honors distinguished individuals with long-standing service, research, and mentoring within the computer vision community.
<ul style="color: #454545; font-size: 18px; font-family: Open Sans; font-weight: 400; line-height: 1.7em;">
<li><strong>2024 Recipient</strong>: Andrea Vedaldi</li>
</ul>
</li>
</ul>
<p>“The TCPAMI Awards demonstrate the lasting impact and influence of CVPR research and researchers,” said Walter J. Scheirer, University of Notre Dame, Notre Dame, Ind., U.S.A., and CVPR 2024 General Chair. “The contributions of these leaders have helped to shape and drive forward continued advancements in the field. We are proud to recognize these achievements and congratulate them on their success.”</p>
<p style="color: #454545; font-size: 18px; font-family: Open Sans; font-weight: 400; line-height: 1.7em;"><strong>About the CVPR 2024</strong><br>
The Computer Vision and Pattern Recognition Conference (CVPR) is the preeminent computer vision event for new research in support of artificial intelligence (AI), machine learning (ML), augmented, virtual and mixed reality (AR/VR/MR), deep learning, and much more. Sponsored by the IEEE Computer Society (CS) and the Computer Vision Foundation (CVF), CVPR delivers the important advances in all areas of computer vision and pattern recognition and the various fields and industries they impact. With a first-in-class technical program, including tutorials and workshops, a leading-edge expo, and robust networking opportunities, CVPR, which is annually attended by more than 10,000 scientists and engineers, creates a one-of-a-kind opportunity for networking, recruiting, inspiration, and motivation.</p>
<p style="color: #454545; font-size: 18px; font-family: Open Sans; font-weight: 400; line-height: 1.7em;">CVPR 2024 takes place 17-21 June at the Seattle Convention Center in Seattle, Wash., U.S.A., and participants may also access sessions virtually. For more information about CVPR 2024, visit <a href="https://cvpr.thecvf.com/" target="_blank" rel="noopener" data-feathr-click-track="true" data-feathr-link-aids="5cdda43ba3a493000bf82f7f">cvpr.thecvf.com</a>.</p>
<p style="color: #454545; font-size: 18px; font-family: Open Sans; font-weight: 400; line-height: 1.7em;"><strong>About the Computer Vision Foundation</strong><br>
The Computer Vision Foundation (CVF) is a non-profit organization whose purpose is to foster and support research on all aspects of computer vision. Together with the IEEE Computer Society, it co-sponsors the two largest computer vision conferences, CVPR and the International Conference on Computer Vision (ICCV). Visit <a href="https://www.thecvf.com/" target="_blank" rel="noopener" data-feathr-click-track="true" data-feathr-link-aids="5cdda43ba3a493000bf82f7f">thecvf.com</a> for more information.</p>
<p style="color: #454545; font-size: 18px; font-family: Open Sans; font-weight: 400; line-height: 1.7em;"><strong>About the IEEE Computer Society</strong><br>
Engaging computer engineers, scientists, academia, and industry professionals from all areas and levels of computing, the IEEE Computer Society (CS) serves as the world’s largest and most established professional organization of its type. IEEE CS sets the standard for the education and engagement that fuels continued global technological advancement. Through conferences, publications, and programs that inspire dialogue, debate, and collaboration, IEEE CS empowers, shapes, and guides the future of not only its 375,000+ community members, but the greater industry, enabling new opportunities to better serve our world. Visit <a href="https://www.computer.org/" target="_blank" rel="noopener" data-feathr-click-track="true" data-feathr-link-aids="5cdda43ba3a493000bf82f7f">computer.org</a> for more information.</p>

</div>








</div>
</div>