<!-- src: https://icml.cc/virtual/2021/awards_detail -->
<div class="container">

<div></div>

<table>

<tbody><tr>
<td>

<div>Outstanding Paper</div>

</td>
<td>

<div class="displaycards touchup-date" id="event-10175">




<div style="width:80%;margin:auto;">
<a class="small-title" href="/virtual/2021/poster/10175">Unbiased Gradient Estimation in Unrolled Computation Graphs with Persistent Evolution Strategies</a>
</div>
<div class="type_display_name_minus_type"></div>
<div class="author-str">Paul Vicol · Luke Metz · Jascha Sohl-Dickstein</div>
<div class="author-str higher"></div>
<div class="text-muted touchup-date-div" id="touchup-date-event-10175">Wed 21 Jul 04:00 AM UTC</div>


<p style="font-size:.9em;">[ Virtual ]</p>






<div class="abstract-section">

<div>
<a id="abstract-link-10175" class="abstract-link" data-toggle="collapse" href="#collapse-event-abstract-10175" role="button" aria-expanded="false" aria-controls="collapse-event-abstract-10175">
Abstract <i id="caret-10175" class="fas fa-caret-right" aria-hidden="true"></i>
</a>
</div>


</div>
<div class="collapse" id="collapse-event-abstract-10175">
<div class="abstract-display">
<p>Unrolled computation graphs arise in many scenarios, including training RNNs, tuning hyperparameters through unrolled optimization, and training learned optimizers. Current approaches to optimizing parameters in such computation graphs suffer from high variance gradients, bias, slow updates, or large memory usage. We introduce a method called Persistent Evolution Strategies (PES), which divides the computation graph into a series of truncated unrolls, and performs an evolution strategies-based update step after each unroll. PES eliminates bias from these truncations by accumulating correction terms over the entire sequence of unrolls. PES allows for rapid parameter updates, has low memory usage, is unbiased, and has reasonable variance characteristics. We experimentally demonstrate the advantages of PES compared to several other methods for gradient estimation on synthetic tasks, and show its applicability to training learned optimizers and tuning hyperparameters.</p>

</div>
</div>

</div>
</td>

</tr>

<tr>
<td>

<div>Outstanding Paper</div>

</td>
<td>

<div class="displaycards touchup-date" id="event-10176">




<div style="width:80%;margin:auto;">
<a class="small-title" href="/virtual/2021/oral/10176">Unbiased Gradient Estimation in Unrolled Computation Graphs with Persistent Evolution Strategies</a>
</div>
<div class="type_display_name_minus_type"></div>
<div class="author-str">Paul Vicol · Luke Metz · Jascha Sohl-Dickstein</div>
<div class="author-str higher"></div>
<div class="text-muted touchup-date-div" id="touchup-date-event-10176">Wed 21 Jul 01:00 AM UTC</div>







<div class="abstract-section">

<div>
<a id="abstract-link-10176" class="abstract-link" data-toggle="collapse" href="#collapse-event-abstract-10176" role="button" aria-expanded="false" aria-controls="collapse-event-abstract-10176">
Abstract <i id="caret-10176" class="fas fa-caret-right" aria-hidden="true"></i>
</a>
</div>


</div>
<div class="collapse" id="collapse-event-abstract-10176">
<div class="abstract-display">
<p>Unrolled computation graphs arise in many scenarios, including training RNNs, tuning hyperparameters through unrolled optimization, and training learned optimizers. Current approaches to optimizing parameters in such computation graphs suffer from high variance gradients, bias, slow updates, or large memory usage. We introduce a method called Persistent Evolution Strategies (PES), which divides the computation graph into a series of truncated unrolls, and performs an evolution strategies-based update step after each unroll. PES eliminates bias from these truncations by accumulating correction terms over the entire sequence of unrolls. PES allows for rapid parameter updates, has low memory usage, is unbiased, and has reasonable variance characteristics. We experimentally demonstrate the advantages of PES compared to several other methods for gradient estimation on synthetic tasks, and show its applicability to training learned optimizers and tuning hyperparameters.</p>

</div>
</div>

</div>
</td>

</tr>

<tr>
<td>

<div>Outstanding Paper Honorable Mention</div>

</td>
<td>

<div class="displaycards touchup-date" id="event-9336">




<div style="width:80%;margin:auto;">
<a class="small-title" href="/virtual/2021/oral/9336">Oops I Took A Gradient: Scalable Sampling for Discrete Distributions</a>
</div>
<div class="type_display_name_minus_type"></div>
<div class="author-str">Will Grathwohl · Kevin Swersky · Milad Hashemi · David Duvenaud · Chris Maddison</div>
<div class="author-str higher"></div>
<div class="text-muted touchup-date-div" id="touchup-date-event-9336">Tue 20 Jul 01:00 PM UTC</div>







<div class="abstract-section">

<div>
<a id="abstract-link-9336" class="abstract-link" data-toggle="collapse" href="#collapse-event-abstract-9336" role="button" aria-expanded="false" aria-controls="collapse-event-abstract-9336">
Abstract <i id="caret-9336" class="fas fa-caret-right" aria-hidden="true"></i>
</a>
</div>


</div>
<div class="collapse" id="collapse-event-abstract-9336">
<div class="abstract-display">
<p>We propose a general and scalable approximate sampling strategy for probabilistic models with discrete variables.  Our approach uses gradients of the likelihood function with respect to its discrete inputs to propose updates in a Metropolis-Hastings sampler. We show empirically that this approach outperforms generic samplers in a number of difficult settings including Ising models, Potts models, restricted Boltzmann machines, and factorial hidden Markov models. We also demonstrate our improved sampler for training deep energy-based models on high dimensional discrete image data. This approach outperforms variational auto-encoders and existing energy-based models.  Finally, we give bounds showing that our approach is near-optimal in the class of samplers which propose local updates.</p>

</div>
</div>

</div>
</td>

</tr>

<tr>
<td>

<div>Outstanding Paper Honorable Mention</div>

</td>
<td>

<div class="displaycards touchup-date" id="event-9335">




<div style="width:80%;margin:auto;">
<a class="small-title" href="/virtual/2021/poster/9335">Oops I Took A Gradient: Scalable Sampling for Discrete Distributions</a>
</div>
<div class="type_display_name_minus_type"></div>
<div class="author-str">Will Grathwohl · Kevin Swersky · Milad Hashemi · David Duvenaud · Chris Maddison</div>
<div class="author-str higher"></div>
<div class="text-muted touchup-date-div" id="touchup-date-event-9335">Tue 20 Jul 04:00 PM UTC</div>


<p style="font-size:.9em;">[ Virtual ]</p>






<div class="abstract-section">

<div>
<a id="abstract-link-9335" class="abstract-link" data-toggle="collapse" href="#collapse-event-abstract-9335" role="button" aria-expanded="false" aria-controls="collapse-event-abstract-9335">
Abstract <i id="caret-9335" class="fas fa-caret-right" aria-hidden="true"></i>
</a>
</div>


</div>
<div class="collapse" id="collapse-event-abstract-9335">
<div class="abstract-display">
<p>We propose a general and scalable approximate sampling strategy for probabilistic models with discrete variables.  Our approach uses gradients of the likelihood function with respect to its discrete inputs to propose updates in a Metropolis-Hastings sampler. We show empirically that this approach outperforms generic samplers in a number of difficult settings including Ising models, Potts models, restricted Boltzmann machines, and factorial hidden Markov models. We also demonstrate our improved sampler for training deep energy-based models on high dimensional discrete image data. This approach outperforms variational auto-encoders and existing energy-based models.  Finally, we give bounds showing that our approach is near-optimal in the class of samplers which propose local updates.</p>

</div>
</div>

</div>
</td>

</tr>

<tr>
<td>

<div>Outstanding Paper Honorable Mention</div>

</td>
<td>

<div class="displaycards touchup-date" id="event-9928">




<div style="width:80%;margin:auto;">
<a class="small-title" href="/virtual/2021/oral/9928">Solving high-dimensional parabolic PDEs using the tensor train format</a>
</div>
<div class="type_display_name_minus_type"></div>
<div class="author-str">Lorenz Richter · Leon Sallandt · Nikolas Nüsken</div>
<div class="author-str higher"></div>
<div class="text-muted touchup-date-div" id="touchup-date-event-9928">Fri 23 Jul 01:00 AM UTC</div>







<div class="abstract-section">

<div>
<a id="abstract-link-9928" class="abstract-link" data-toggle="collapse" href="#collapse-event-abstract-9928" role="button" aria-expanded="false" aria-controls="collapse-event-abstract-9928">
Abstract <i id="caret-9928" class="fas fa-caret-right" aria-hidden="true"></i>
</a>
</div>


</div>
<div class="collapse" id="collapse-event-abstract-9928">
<div class="abstract-display">
<p>High-dimensional partial differential equations (PDEs) are ubiquitous in economics, science and engineering. However, their numerical treatment poses formidable challenges since traditional grid-based methods tend to be frustrated by the curse of dimensionality. In this paper, we argue that tensor trains provide an appealing approximation framework for parabolic PDEs: the combination of reformulations in terms of backward stochastic differential equations and regression-type methods in the tensor format holds the promise of leveraging latent low-rank structures enabling both compression and efficient computation. Following this paradigm, we develop novel iterative schemes, involving either explicit and fast or implicit and accurate updates. We demonstrate in a number of examples that our methods achieve a favorable trade-off between accuracy and computational efficiency in comparison with state-of-the-art neural network based approaches.</p>

</div>
</div>

</div>
</td>

</tr>

<tr>
<td>

<div>Outstanding Paper Honorable Mention</div>

</td>
<td>

<div class="displaycards touchup-date" id="event-8894">




<div style="width:80%;margin:auto;">
<a class="small-title" href="/virtual/2021/oral/8894">Optimal Complexity in Decentralized Training</a>
</div>
<div class="type_display_name_minus_type"></div>
<div class="author-str">Yucheng Lu · Christopher De Sa</div>
<div class="author-str higher"></div>
<div class="text-muted touchup-date-div" id="touchup-date-event-8894">Tue 20 Jul 12:00 PM UTC</div>







<div class="abstract-section">

<div>
<a id="abstract-link-8894" class="abstract-link" data-toggle="collapse" href="#collapse-event-abstract-8894" role="button" aria-expanded="false" aria-controls="collapse-event-abstract-8894">
Abstract <i id="caret-8894" class="fas fa-caret-right" aria-hidden="true"></i>
</a>
</div>


</div>
<div class="collapse" id="collapse-event-abstract-8894">
<div class="abstract-display">
<p>Decentralization is a promising method of scaling up parallel machine learning systems. In this paper, we provide a tight lower bound on the iteration complexity for such methods in a stochastic non-convex setting. Our lower bound reveals a theoretical gap in known convergence rates of many existing decentralized training algorithms, such as D-PSGD.  We prove by construction this lower bound is tight and achievable. Motivated by our insights, we further propose DeTAG, a practical gossip-style decentralized algorithm that achieves the lower bound with only a logarithm gap. Empirically, we compare DeTAG with other decentralized algorithms on image classification tasks, and we show DeTAG enjoys faster convergence compared to baselines, especially on unshuffled data and in sparse networks.</p>

</div>
</div>

</div>
</td>

</tr>

<tr>
<td>

<div>Outstanding Paper Honorable Mention</div>

</td>
<td>

<div class="displaycards touchup-date" id="event-8893">




<div style="width:80%;margin:auto;">
<a class="small-title" href="/virtual/2021/poster/8893">Optimal Complexity in Decentralized Training</a>
</div>
<div class="type_display_name_minus_type"></div>
<div class="author-str">Yucheng Lu · Christopher De Sa</div>
<div class="author-str higher"></div>
<div class="text-muted touchup-date-div" id="touchup-date-event-8893">Tue 20 Jul 04:00 PM UTC</div>







<div class="abstract-section">

<div>
<a id="abstract-link-8893" class="abstract-link" data-toggle="collapse" href="#collapse-event-abstract-8893" role="button" aria-expanded="false" aria-controls="collapse-event-abstract-8893">
Abstract <i id="caret-8893" class="fas fa-caret-right" aria-hidden="true"></i>
</a>
</div>


</div>
<div class="collapse" id="collapse-event-abstract-8893">
<div class="abstract-display">
<p>Decentralization is a promising method of scaling up parallel machine learning systems. In this paper, we provide a tight lower bound on the iteration complexity for such methods in a stochastic non-convex setting. Our lower bound reveals a theoretical gap in known convergence rates of many existing decentralized training algorithms, such as D-PSGD.  We prove by construction this lower bound is tight and achievable. Motivated by our insights, we further propose DeTAG, a practical gossip-style decentralized algorithm that achieves the lower bound with only a logarithm gap. Empirically, we compare DeTAG with other decentralized algorithms on image classification tasks, and we show DeTAG enjoys faster convergence compared to baselines, especially on unshuffled data and in sparse networks.</p>

</div>
</div>

</div>
</td>

</tr>

<tr>
<td>

<div>Outstanding Paper Honorable Mention</div>

</td>
<td>

<div class="displaycards touchup-date" id="event-9927">




<div style="width:80%;margin:auto;">
<a class="small-title" href="/virtual/2021/poster/9927">Solving high-dimensional parabolic PDEs using the tensor train format</a>
</div>
<div class="type_display_name_minus_type"></div>
<div class="author-str">Lorenz Richter · Leon Sallandt · Nikolas Nüsken</div>
<div class="author-str higher"></div>
<div class="text-muted touchup-date-div" id="touchup-date-event-9927">Fri 23 Jul 04:00 AM UTC</div>







<div class="abstract-section">

<div>
<a id="abstract-link-9927" class="abstract-link" data-toggle="collapse" href="#collapse-event-abstract-9927" role="button" aria-expanded="false" aria-controls="collapse-event-abstract-9927">
Abstract <i id="caret-9927" class="fas fa-caret-right" aria-hidden="true"></i>
</a>
</div>


</div>
<div class="collapse" id="collapse-event-abstract-9927">
<div class="abstract-display">
<p>High-dimensional partial differential equations (PDEs) are ubiquitous in economics, science and engineering. However, their numerical treatment poses formidable challenges since traditional grid-based methods tend to be frustrated by the curse of dimensionality. In this paper, we argue that tensor trains provide an appealing approximation framework for parabolic PDEs: the combination of reformulations in terms of backward stochastic differential equations and regression-type methods in the tensor format holds the promise of leveraging latent low-rank structures enabling both compression and efficient computation. Following this paradigm, we develop novel iterative schemes, involving either explicit and fast or implicit and accurate updates. We demonstrate in a number of examples that our methods achieve a favorable trade-off between accuracy and computational efficiency in comparison with state-of-the-art neural network based approaches.</p>

</div>
</div>

</div>
</td>

</tr>

<tr>
<td>

<div>Outstanding Paper Honorable Mention</div>

</td>
<td>

<div class="displaycards touchup-date" id="event-10403">




<div style="width:80%;margin:auto;">
<a class="small-title" href="/virtual/2021/poster/10403">Understanding self-supervised learning dynamics without contrastive pairs</a>
</div>
<div class="type_display_name_minus_type"></div>
<div class="author-str">Yuandong Tian · Xinlei Chen · Surya Ganguli</div>
<div class="author-str higher"></div>
<div class="text-muted touchup-date-div" id="touchup-date-event-10403">Thu 22 Jul 04:00 AM UTC</div>


<p style="font-size:.9em;">[ Virtual ]</p>






<div class="abstract-section">

<div>
<a id="abstract-link-10403" class="abstract-link" data-toggle="collapse" href="#collapse-event-abstract-10403" role="button" aria-expanded="false" aria-controls="collapse-event-abstract-10403">
Abstract <i id="caret-10403" class="fas fa-caret-right" aria-hidden="true"></i>
</a>
</div>


</div>
<div class="collapse" id="collapse-event-abstract-10403">
<div class="abstract-display">
While contrastive approaches of self-supervised learning (SSL) learn representations by minimizing the distance between two augmented views of the same data point (positive pairs) and maximizing views from different data points (negative pairs), recent \emph{non-contrastive} SSL (e.g., BYOL and SimSiam) show remarkable performance {\it without} negative pairs, with an extra learnable predictor and a stop-gradient operation. A fundamental question rises: why they do not collapse into trivial representation? In this paper, we answer this question via a simple theoretical study and propose a novel approach, \ourmethod{}, that \emph{directly} sets the linear predictor based on the statistics of its inputs, rather than trained with gradient update. On ImageNet, it performs comparably with more complex two-layer non-linear predictors that employ BatchNorm and outperforms linear predictor by <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-1-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>2.5</mn><mi mathvariant=&quot;normal&quot;>&amp;#x0025;</mi></math>" role="presentation"><span id="MJXc-Node-1" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-2" class="mjx-mrow"><span id="MJXc-Node-3" class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.378em; padding-bottom: 0.378em;">2.5</span></span><span id="MJXc-Node-4" class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.441em; padding-bottom: 0.378em;">%</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>2.5</mn><mi mathvariant="normal">%</mi></math></span></span><script type="math/tex" id="MathJax-Element-1">2.5\%</script> in 300-epoch training (and <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-2-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>5</mn><mi mathvariant=&quot;normal&quot;>&amp;#x0025;</mi></math>" role="presentation"><span id="MJXc-Node-5" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-6" class="mjx-mrow"><span id="MJXc-Node-7" class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.378em; padding-bottom: 0.378em;">5</span></span><span id="MJXc-Node-8" class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.441em; padding-bottom: 0.378em;">%</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>5</mn><mi mathvariant="normal">%</mi></math></span></span><script type="math/tex" id="MathJax-Element-2">5\%</script> in 60-epoch). \ourmethod{} is motivated by our theoretical study of the nonlinear learning dynamics of non-contrastive SSL in simple linear networks. Our study yields conceptual insights into how non-contrastive SSL methods learn, how they avoid representational collapse, and how multiple factors, like predictor networks, stop-gradients, exponential moving averages, and weight decay all come into play. Our simple theory recapitulates the results of real-world ablation studies in both STL-10 and ImageNet. …
</div>
</div>

</div>
</td>

</tr>

<tr>
<td>

<div>Outstanding Paper Honorable Mention</div>

</td>
<td>

<div class="displaycards touchup-date" id="event-10404">




<div style="width:80%;margin:auto;">
<a class="small-title" href="/virtual/2021/oral/10404">Understanding self-supervised learning dynamics without contrastive pairs</a>
</div>
<div class="type_display_name_minus_type"></div>
<div class="author-str">Yuandong Tian · Xinlei Chen · Surya Ganguli</div>
<div class="author-str higher"></div>
<div class="text-muted touchup-date-div" id="touchup-date-event-10404">Thu 22 Jul 12:00 AM UTC</div>







<div class="abstract-section">

<div>
<a id="abstract-link-10404" class="abstract-link" data-toggle="collapse" href="#collapse-event-abstract-10404" role="button" aria-expanded="false" aria-controls="collapse-event-abstract-10404">
Abstract <i id="caret-10404" class="fas fa-caret-right" aria-hidden="true"></i>
</a>
</div>


</div>
<div class="collapse" id="collapse-event-abstract-10404">
<div class="abstract-display">
While contrastive approaches of self-supervised learning (SSL) learn representations by minimizing the distance between two augmented views of the same data point (positive pairs) and maximizing views from different data points (negative pairs), recent \emph{non-contrastive} SSL (e.g., BYOL and SimSiam) show remarkable performance {\it without} negative pairs, with an extra learnable predictor and a stop-gradient operation. A fundamental question rises: why they do not collapse into trivial representation? In this paper, we answer this question via a simple theoretical study and propose a novel approach, \ourmethod{}, that \emph{directly} sets the linear predictor based on the statistics of its inputs, rather than trained with gradient update. On ImageNet, it performs comparably with more complex two-layer non-linear predictors that employ BatchNorm and outperforms linear predictor by <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-3-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>2.5</mn><mi mathvariant=&quot;normal&quot;>&amp;#x0025;</mi></math>" role="presentation"><span id="MJXc-Node-9" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-10" class="mjx-mrow"><span id="MJXc-Node-11" class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.378em; padding-bottom: 0.378em;">2.5</span></span><span id="MJXc-Node-12" class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.441em; padding-bottom: 0.378em;">%</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>2.5</mn><mi mathvariant="normal">%</mi></math></span></span><script type="math/tex" id="MathJax-Element-3">2.5\%</script> in 300-epoch training (and <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-4-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>5</mn><mi mathvariant=&quot;normal&quot;>&amp;#x0025;</mi></math>" role="presentation"><span id="MJXc-Node-13" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-14" class="mjx-mrow"><span id="MJXc-Node-15" class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.378em; padding-bottom: 0.378em;">5</span></span><span id="MJXc-Node-16" class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.441em; padding-bottom: 0.378em;">%</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>5</mn><mi mathvariant="normal">%</mi></math></span></span><script type="math/tex" id="MathJax-Element-4">5\%</script> in 60-epoch). \ourmethod{} is motivated by our theoretical study of the nonlinear learning dynamics of non-contrastive SSL in simple linear networks. Our study yields conceptual insights into how non-contrastive SSL methods learn, how they avoid representational collapse, and how multiple factors, like predictor networks, stop-gradients, exponential moving averages, and weight decay all come into play. Our simple theory recapitulates the results of real-world ablation studies in both STL-10 and ImageNet. …
</div>
</div>

</div>
</td>

</tr>

</tbody></table>


<form method="POST"><input type="hidden" name="csrfmiddlewaretoken" value="dbgB3Zcp1IQUBjLgSlb1X0ihxKBPbU6JMo7Ll7EGZgywkbab0FpO12xwFd47GvdP"></form>
</div>