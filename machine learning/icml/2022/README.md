<!-- src: https://icml.cc/virtual/2022/awards_detail -->
<div class="container">

<div></div>

<table>

<tbody><tr>
<td>

<div>Test of Time Award</div>

</td>
<td>

<div class="displaycards touchup-date" id="event-20803">




<div style="width:80%;margin:auto;">
<a class="small-title" href="/virtual/2022/award/20803">Test of Time Award</a>
</div>
<div class="type_display_name_minus_type"></div>
<div class="author-str"></div>
<div class="author-str higher"></div>
<div class="text-muted touchup-date-div" id="touchup-date-event-20803">Tue 19 Jul 07:30 PM UTC</div>


<p style="font-size:.9em;">[ Hall F ]</p>





<div class="abstract-section">

<div>
<a id="abstract-link-20803" class="abstract-link" data-toggle="collapse" href="#collapse-event-abstract-20803" role="button" aria-expanded="false" aria-controls="collapse-event-abstract-20803">
Abstract <i id="caret-20803" class="fas fa-caret-right" aria-hidden="true"></i>
</a>
</div>


</div>
<div class="collapse" id="collapse-event-abstract-20803">
<div class="abstract-display">
<p>&nbsp;</p>

<h4>Test of Time Award:</h4>

<p><strong>Poisoning Attacks Against Support Vector Machines</strong></p>

<p><i>Battista Biggio, Blaine Nelson, Pavel Laskov:</i></p>

<hr>

<h4>Test of Time Honorable Mention:</h4>

<p><strong>Building high-level features using large scale unsupervised learning</strong></p>

<p><i>Quoc Le, Marc'Aurelio Ranzato, Rajat Monga, Matthieu Devin, Kai Chen, Greg Corrado, Jeff Dean, Andrew Ng</i></p>

<p><strong>On causal and anticausal learning</strong></p>

<p><i>Bernhard Schölkopf, Dominik Janzing, Jonas Peters, Eleni Sgouritsa, 
Kun Zhang, Joris Mooij</i></p>

</div>
</div>

</div>
</td>

</tr>

<tr>
<td>

<div>Outstanding Paper</div>

</td>
<td>

<div class="displaycards touchup-date" id="event-17122">




<div style="width:80%;margin:auto;">
<a class="small-title" href="/virtual/2022/oral/17122">Causal Conceptions of Fairness and their Consequences</a>
</div>
<div class="type_display_name_minus_type"></div>
<div class="author-str">Hamed Nilforoshan · Johann Gaebler · Ravi Shroff · Sharad Goel</div>
<div class="author-str higher"></div>
<div class="text-muted touchup-date-div" id="touchup-date-event-17122">Thu 21 Jul 06:05 PM UTC</div>


<p style="font-size:.9em;">[ Room 307 ]</p>






<div class="abstract-section">

<div>
<a id="abstract-link-17122" class="abstract-link" data-toggle="collapse" href="#collapse-event-abstract-17122" role="button" aria-expanded="false" aria-controls="collapse-event-abstract-17122">
Abstract <i id="caret-17122" class="fas fa-caret-right" aria-hidden="true"></i>
</a>
</div>


</div>
<div class="collapse" id="collapse-event-abstract-17122">
<div class="abstract-display">
<p>Recent work highlights the role of causality in designing equitable decision-making algorithms. It is not immediately clear, however, how existing causal conceptions of fairness relate to one another, or what the consequences are of using these definitions as design principles. Here, we first assemble and categorize popular causal definitions of algorithmic fairness into two broad families: (1) those that constrain the effects of decisions on counterfactual disparities; and (2) those that constrain the effects of legally protected characteristics, like race and gender, on decisions. We then show, analytically and empirically, that both families of definitions \emph{almost always}---in a measure theoretic sense---result in strongly Pareto dominated decision policies, meaning there is an alternative, unconstrained policy favored by every stakeholder with preferences drawn from a large, natural class. For example, in the case of college admissions decisions, policies constrained to satisfy causal fairness definitions would be disfavored by every stakeholder with neutral or positive preferences for both academic preparedness and diversity.Indeed, under a prominent definition of causal fairness, we prove the resulting policies require admitting all students with the same probability, regardless of academic qualifications or group membership. Our results highlight formal limitations and potential adverse consequences of common mathematical notions of …</p>
</div>
</div>

</div>
</td>

</tr>

<tr>
<td>

<div>Outstanding Paper</div>

</td>
<td>

<div class="displaycards touchup-date" id="event-16290">




<div style="width:80%;margin:auto;">
<a class="small-title" href="/virtual/2022/oral/16290">The Importance of Non-Markovianity in Maximum State Entropy Exploration</a>
</div>
<div class="type_display_name_minus_type"></div>
<div class="author-str">Mirco Mutti · Riccardo De Santi · Marcello Restelli</div>
<div class="author-str higher"></div>
<div class="text-muted touchup-date-div" id="touchup-date-event-16290">Thu 21 Jul 02:30 PM UTC</div>


<p style="font-size:.9em;">[ Hall G ]</p>






<div class="abstract-section">

<div>
<a id="abstract-link-16290" class="abstract-link" data-toggle="collapse" href="#collapse-event-abstract-16290" role="button" aria-expanded="false" aria-controls="collapse-event-abstract-16290">
Abstract <i id="caret-16290" class="fas fa-caret-right" aria-hidden="true"></i>
</a>
</div>


</div>
<div class="collapse" id="collapse-event-abstract-16290">
<div class="abstract-display">
<p>In the maximum state entropy exploration framework, an agent interacts with a reward-free environment to learn a policy that maximizes the entropy of the expected state visitations it is inducing. Hazan et al. (2019) noted that the class of Markovian stochastic policies is sufficient for the maximum state entropy objective, and exploiting non-Markovianity is generally considered pointless in this setting. In this paper, we argue that non-Markovianity is instead paramount for maximum state entropy exploration in a finite-sample regime. Especially, we recast the objective to target the expected entropy of the induced state visitations in a single trial. Then, we show that the class of non-Markovian deterministic policies is sufficient for the introduced objective, while Markovian policies suffer non-zero regret in general. However, we prove that the problem of finding an optimal non-Markovian policy is NP-hard. Despite this negative result, we discuss avenues to address the problem in a tractable way and how non-Markovian exploration could benefit the sample efficiency of online reinforcement learning in future works.</p>

</div>
</div>

</div>
</td>

</tr>

<tr>
<td>

<div>Outstanding Paper</div>

</td>
<td>

<div class="displaycards touchup-date" id="event-17991">




<div style="width:80%;margin:auto;">
<a class="small-title" href="/virtual/2022/poster/17991">Bayesian Model Selection, the Marginal Likelihood, and Generalization</a>
</div>
<div class="type_display_name_minus_type"></div>
<div class="author-str">Sanae Lotfi · Pavel Izmailov · Gregory Benton · Micah Goldblum · Andrew Wilson</div>
<div class="author-str higher"></div>
<div class="text-muted touchup-date-div" id="touchup-date-event-17991">Thu 21 Jul 10:00 PM UTC</div>


<p style="font-size:.9em;">[ Hall E ]</p>






<div class="abstract-section">

<div>
<a id="abstract-link-17991" class="abstract-link" data-toggle="collapse" href="#collapse-event-abstract-17991" role="button" aria-expanded="false" aria-controls="collapse-event-abstract-17991">
Abstract <i id="caret-17991" class="fas fa-caret-right" aria-hidden="true"></i>
</a>
</div>


</div>
<div class="collapse" id="collapse-event-abstract-17991">
<div class="abstract-display">
<p>How do we compare between hypotheses that are entirely consistent with observations? The marginal likelihood (aka Bayesian evidence), which represents the probability of generating our observations from a prior, provides a distinctive approach to this foundational question, automatically encoding Occam's razor. Although it has been observed that the marginal likelihood can overfit and is sensitive to prior assumptions, its limitations for hyperparameter learning and discrete model comparison have not been thoroughly investigated. We first revisit the appealing properties of the marginal likelihood for learning constraints and hypothesis testing. We then highlight the conceptual and practical issues in using the marginal likelihood as a proxy for generalization. Namely, we show how marginal likelihood can be negatively correlated with generalization, with implications for neural architecture search, and can lead to both underfitting and overfitting in hyperparameter learning. We provide a partial remedy through a conditional marginal likelihood, which we show is more aligned with generalization, and practically valuable for large-scale hyperparameter learning, such as in deep kernel learning.</p>

</div>
</div>

</div>
</td>

</tr>

<tr>
<td>

<div>Outstanding Paper</div>

</td>
<td>

<div class="displaycards touchup-date" id="event-16666">




<div style="width:80%;margin:auto;">
<a class="small-title" href="/virtual/2022/oral/16666">G-Mixup: Graph Data Augmentation for Graph Classification</a>
</div>
<div class="type_display_name_minus_type"></div>
<div class="author-str">Xiaotian Han · Zhimeng Jiang · Ninghao Liu · Xia Hu</div>
<div class="author-str higher"></div>
<div class="text-muted touchup-date-div" id="touchup-date-event-16666">Thu 21 Jul 03:00 PM UTC</div>


<p style="font-size:.9em;">[ Room 318 - 320 ]</p>






<div class="abstract-section">

<div>
<a id="abstract-link-16666" class="abstract-link" data-toggle="collapse" href="#collapse-event-abstract-16666" role="button" aria-expanded="false" aria-controls="collapse-event-abstract-16666">
Abstract <i id="caret-16666" class="fas fa-caret-right" aria-hidden="true"></i>
</a>
</div>


</div>
<div class="collapse" id="collapse-event-abstract-16666">
<div class="abstract-display">
<p>This work develops mixup for graph data. Mixup has shown superiority in improving the generalization and robustness of neural networks by interpolating features and labels between two random samples. Traditionally, Mixup can work on regular, grid-like, and Euclidean data such as image or tabular data. However, it is challenging to directly adopt Mixup to augment graph data because different graphs typically: 1) have different numbers of nodes; 2) are not readily aligned; and 3) have unique typologies in non-Euclidean space. To this end, we propose G-Mixup to augment graphs for graph classification by interpolating the generator (i.e., graphon) of different classes of graphs. Specifically, we first use graphs within the same class to estimate a graphon. Then, instead of directly manipulating graphs, we interpolate graphons of different classes in the Euclidean space to get mixed graphons, where the synthetic graphs are generated through sampling based on the mixed graphons. Extensive experiments show that G-Mixup substantially improves the generalization and robustness of GNNs.</p>

</div>
</div>

</div>
</td>

</tr>

<tr>
<td>

<div>Outstanding Paper</div>

</td>
<td>

<div class="displaycards touchup-date" id="event-16769">




<div style="width:80%;margin:auto;">
<a class="small-title" href="/virtual/2022/poster/16769">Do Differentiable Simulators Give Better Policy Gradients?</a>
</div>
<div class="type_display_name_minus_type"></div>
<div class="author-str">Hyung Ju Suh · Max Simchowitz · Kaiqing Zhang · Russ Tedrake</div>
<div class="author-str higher"></div>
<div class="text-muted touchup-date-div" id="touchup-date-event-16769">Thu 21 Jul 10:00 PM UTC</div>


<p style="font-size:.9em;">[ Hall E ]</p>






<div class="abstract-section">

<div>
<a id="abstract-link-16769" class="abstract-link" data-toggle="collapse" href="#collapse-event-abstract-16769" role="button" aria-expanded="false" aria-controls="collapse-event-abstract-16769">
Abstract <i id="caret-16769" class="fas fa-caret-right" aria-hidden="true"></i>
</a>
</div>


</div>
<div class="collapse" id="collapse-event-abstract-16769">
<div class="abstract-display">
Differentiable simulators promise faster computation time for reinforcement learning by replacing zeroth-order gradient estimates of a stochastic objective with an estimate based on first-order gradients. However, it is yet unclear what factors decide the performance of the two estimators on complex landscapes that involve long-horizon planning and control on physical systems, despite the crucial relevance of this question for the utility of differentiable simulators. We show that characteristics of certain physical systems, such as stiffness or discontinuities, may compromise the efficacy of the first-order estimator, and analyze this phenomenon through the lens of bias and variance. We additionally propose an <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-1-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>" role="presentation"><span id="MJXc-Node-1" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-2" class="mjx-mrow"><span id="MJXc-Node-3" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">α</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>α</mi></math></span></span><script type="math/tex" id="MathJax-Element-1">\alpha</script>-order gradient estimator, with <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-2-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi><mo>&amp;#x2208;</mo><mo stretchy=&quot;false&quot;>[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=&quot;false&quot;>]</mo></math>" role="presentation"><span id="MJXc-Node-4" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-5" class="mjx-mrow"><span id="MJXc-Node-6" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">α</span></span><span id="MJXc-Node-7" class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.253em; padding-bottom: 0.378em;">∈</span></span><span id="MJXc-Node-8" class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.441em; padding-bottom: 0.566em;">[</span></span><span id="MJXc-Node-9" class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.378em; padding-bottom: 0.378em;">0</span></span><span id="MJXc-Node-10" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.184em; padding-bottom: 0.566em;">,</span></span><span id="MJXc-Node-11" class="mjx-mn MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.378em; padding-bottom: 0.316em;">1</span></span><span id="MJXc-Node-12" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.441em; padding-bottom: 0.566em;">]</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>α</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></math></span></span><script type="math/tex" id="MathJax-Element-2">\alpha \in [0,1]</script>, which correctly utilizes exact gradients to combine the efficiency of first-order estimates with the robustness of zero-order methods. We demonstrate the pitfalls of traditional estimators and the advantages of the <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-3-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>" role="presentation"><span id="MJXc-Node-13" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-14" class="mjx-mrow"><span id="MJXc-Node-15" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">α</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>α</mi></math></span></span><script type="math/tex" id="MathJax-Element-3">\alpha</script>-order estimator on some numerical examples.
</div>
</div>

</div>
</td>

</tr>

<tr>
<td>

<div>Outstanding Paper</div>

</td>
<td>

<div class="displaycards touchup-date" id="event-17992">




<div style="width:80%;margin:auto;">
<a class="small-title" href="/virtual/2022/oral/17992">Bayesian Model Selection, the Marginal Likelihood, and Generalization</a>
</div>
<div class="type_display_name_minus_type"></div>
<div class="author-str">Sanae Lotfi · Pavel Izmailov · Gregory Benton · Micah Goldblum · Andrew Wilson</div>
<div class="author-str higher"></div>
<div class="text-muted touchup-date-div" id="touchup-date-event-17992">Thu 21 Jul 06:10 PM UTC</div>


<p style="font-size:.9em;">[ Room 310 ]</p>






<div class="abstract-section">

<div>
<a id="abstract-link-17992" class="abstract-link" data-toggle="collapse" href="#collapse-event-abstract-17992" role="button" aria-expanded="false" aria-controls="collapse-event-abstract-17992">
Abstract <i id="caret-17992" class="fas fa-caret-right" aria-hidden="true"></i>
</a>
</div>


</div>
<div class="collapse" id="collapse-event-abstract-17992">
<div class="abstract-display">
<p>How do we compare between hypotheses that are entirely consistent with observations? The marginal likelihood (aka Bayesian evidence), which represents the probability of generating our observations from a prior, provides a distinctive approach to this foundational question, automatically encoding Occam's razor. Although it has been observed that the marginal likelihood can overfit and is sensitive to prior assumptions, its limitations for hyperparameter learning and discrete model comparison have not been thoroughly investigated. We first revisit the appealing properties of the marginal likelihood for learning constraints and hypothesis testing. We then highlight the conceptual and practical issues in using the marginal likelihood as a proxy for generalization. Namely, we show how marginal likelihood can be negatively correlated with generalization, with implications for neural architecture search, and can lead to both underfitting and overfitting in hyperparameter learning. We provide a partial remedy through a conditional marginal likelihood, which we show is more aligned with generalization, and practically valuable for large-scale hyperparameter learning, such as in deep kernel learning.</p>

</div>
</div>

</div>
</td>

</tr>

<tr>
<td>

<div>Outstanding Paper</div>

</td>
<td>

<div class="displaycards touchup-date" id="event-16842">




<div style="width:80%;margin:auto;">
<a class="small-title" href="/virtual/2022/oral/16842">Stable Conformal Prediction Sets</a>
</div>
<div class="type_display_name_minus_type"></div>
<div class="author-str">Eugene Ndiaye</div>
<div class="author-str higher"></div>
<div class="text-muted touchup-date-div" id="touchup-date-event-16842">Wed 20 Jul 03:05 PM UTC</div>


<p style="font-size:.9em;">[ Ballroom 3 &amp; 4 ]</p>






<div class="abstract-section">

<div>
<a id="abstract-link-16842" class="abstract-link" data-toggle="collapse" href="#collapse-event-abstract-16842" role="button" aria-expanded="false" aria-controls="collapse-event-abstract-16842">
Abstract <i id="caret-16842" class="fas fa-caret-right" aria-hidden="true"></i>
</a>
</div>


</div>
<div class="collapse" id="collapse-event-abstract-16842">
<div class="abstract-display">
When one observes a sequence of variables <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-4-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>y</mi><mn>1</mn></msub><mo stretchy=&quot;false&quot;>)</mo><mo>,</mo><mo>&amp;#x2026;</mo><mo>,</mo><mo stretchy=&quot;false&quot;>(</mo><msub><mi>x</mi><mi>n</mi></msub><mo>,</mo><msub><mi>y</mi><mi>n</mi></msub><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation"><span id="MJXc-Node-16" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-17" class="mjx-mrow"><span id="MJXc-Node-18" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.441em; padding-bottom: 0.566em;">(</span></span><span id="MJXc-Node-19" class="mjx-msubsup"><span class="mjx-base"><span id="MJXc-Node-20" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">x</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span id="MJXc-Node-21" class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.378em; padding-bottom: 0.316em;">1</span></span></span></span><span id="MJXc-Node-22" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.184em; padding-bottom: 0.566em;">,</span></span><span id="MJXc-Node-23" class="mjx-msubsup MJXc-space1"><span class="mjx-base" style="margin-right: -0.006em;"><span id="MJXc-Node-24" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.503em; padding-right: 0.006em;">y</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span id="MJXc-Node-25" class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.378em; padding-bottom: 0.316em;">1</span></span></span></span><span id="MJXc-Node-26" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.441em; padding-bottom: 0.566em;">)</span></span><span id="MJXc-Node-27" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.184em; padding-bottom: 0.566em;">,</span></span><span id="MJXc-Node-28" class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.184em; padding-bottom: 0.316em;">…</span></span><span id="MJXc-Node-29" class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.184em; padding-bottom: 0.566em;">,</span></span><span id="MJXc-Node-30" class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.441em; padding-bottom: 0.566em;">(</span></span><span id="MJXc-Node-31" class="mjx-msubsup"><span class="mjx-base"><span id="MJXc-Node-32" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">x</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span id="MJXc-Node-33" class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">n</span></span></span></span><span id="MJXc-Node-34" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.184em; padding-bottom: 0.566em;">,</span></span><span id="MJXc-Node-35" class="mjx-msubsup MJXc-space1"><span class="mjx-base" style="margin-right: -0.006em;"><span id="MJXc-Node-36" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.503em; padding-right: 0.006em;">y</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span id="MJXc-Node-37" class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">n</span></span></span></span><span id="MJXc-Node-38" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.441em; padding-bottom: 0.566em;">)</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>y</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mo>,</mo><mo>…</mo><mo>,</mo><mo stretchy="false">(</mo><msub><mi>x</mi><mi>n</mi></msub><mo>,</mo><msub><mi>y</mi><mi>n</mi></msub><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-4">(x_1, y_1), \ldots, (x_n, y_n)</script>, Conformal Prediction (CP) is a methodology that allows to estimate a confidence set for <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-5-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub></math>" role="presentation"><span id="MJXc-Node-39" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-40" class="mjx-mrow"><span id="MJXc-Node-41" class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.006em;"><span id="MJXc-Node-42" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.503em; padding-right: 0.006em;">y</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span id="MJXc-Node-43" class="mjx-texatom" style=""><span id="MJXc-Node-44" class="mjx-mrow"><span id="MJXc-Node-45" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">n</span></span><span id="MJXc-Node-46" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.316em; padding-bottom: 0.441em;">+</span></span><span id="MJXc-Node-47" class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.378em; padding-bottom: 0.316em;">1</span></span></span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>y</mi><mrow class="MJX-TeXAtom-ORD"><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-5">y_{n+1}</script> given <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-6-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub></math>" role="presentation"><span id="MJXc-Node-48" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-49" class="mjx-mrow"><span id="MJXc-Node-50" class="mjx-msubsup"><span class="mjx-base"><span id="MJXc-Node-51" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">x</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span id="MJXc-Node-52" class="mjx-texatom" style=""><span id="MJXc-Node-53" class="mjx-mrow"><span id="MJXc-Node-54" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">n</span></span><span id="MJXc-Node-55" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.316em; padding-bottom: 0.441em;">+</span></span><span id="MJXc-Node-56" class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.378em; padding-bottom: 0.316em;">1</span></span></span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>x</mi><mrow class="MJX-TeXAtom-ORD"><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-6">x_{n+1}</script> by merely assuming that the distribution of the data is exchangeable. CP sets have guaranteed coverage for any finite population size <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-7-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi></math>" role="presentation"><span id="MJXc-Node-57" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-58" class="mjx-mrow"><span id="MJXc-Node-59" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">n</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></span></span><script type="math/tex" id="MathJax-Element-7">n</script>. While appealing, the computation of such a set turns out to be infeasible in general, \eg when the unknown variable <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-8-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub></math>" role="presentation"><span id="MJXc-Node-60" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-61" class="mjx-mrow"><span id="MJXc-Node-62" class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.006em;"><span id="MJXc-Node-63" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.503em; padding-right: 0.006em;">y</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span id="MJXc-Node-64" class="mjx-texatom" style=""><span id="MJXc-Node-65" class="mjx-mrow"><span id="MJXc-Node-66" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">n</span></span><span id="MJXc-Node-67" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.316em; padding-bottom: 0.441em;">+</span></span><span id="MJXc-Node-68" class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.378em; padding-bottom: 0.316em;">1</span></span></span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>y</mi><mrow class="MJX-TeXAtom-ORD"><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-8">y_{n+1}</script> is continuous. The bottleneck is that it is based on a procedure that readjusts a prediction model on data where we replace the unknown target by all its possible values in order to select the most probable one. This requires computing an infinite number of models, which often makes it intractable. In this paper, we combine CP techniques with classical algorithmic stability bounds to derive a prediction set computable with a single model fit. We demonstrate that our proposed confidence set does not lose any coverage guarantees while avoiding the need for data splitting as currently done in the literature. We provide some numerical experiments to illustrate the tightness of our estimation when the sample size is sufficiently large, on both synthetic and real datasets.
</div>
</div>

</div>
</td>

</tr>

<tr>
<td>

<div>Outstanding Paper</div>

</td>
<td>

<div class="displaycards touchup-date" id="event-15993">




<div style="width:80%;margin:auto;">
<a class="small-title" href="/virtual/2022/poster/15993">Learning Mixtures of Linear Dynamical Systems</a>
</div>
<div class="type_display_name_minus_type"></div>
<div class="author-str">Yanxi Chen · H. Vincent Poor</div>
<div class="author-str higher"></div>
<div class="text-muted touchup-date-div" id="touchup-date-event-15993">Tue 19 Jul 10:30 PM UTC</div>


<p style="font-size:.9em;">[ Hall E ]</p>






<div class="abstract-section">

<div>
<a id="abstract-link-15993" class="abstract-link" data-toggle="collapse" href="#collapse-event-abstract-15993" role="button" aria-expanded="false" aria-controls="collapse-event-abstract-15993">
Abstract <i id="caret-15993" class="fas fa-caret-right" aria-hidden="true"></i>
</a>
</div>


</div>
<div class="collapse" id="collapse-event-abstract-15993">
<div class="abstract-display">
We study the problem of learning a mixture of multiple linear dynamical systems (LDSs) from unlabeled short sample trajectories, each generated by one of the LDS models. Despite the wide applicability of mixture models for time-series data, learning algorithms that come with end-to-end performance guarantees are largely absent from existing literature. There are multiple sources of technical challenges, including but not limited to (1) the presence of latent variables (i.e. the unknown labels of trajectories); (2) the possibility that the sample trajectories might have lengths much smaller than the dimension <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-9-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi></math>" role="presentation"><span id="MJXc-Node-69" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-70" class="mjx-mrow"><span id="MJXc-Node-71" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.503em; padding-bottom: 0.316em; padding-right: 0.003em;">d</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>d</mi></math></span></span><script type="math/tex" id="MathJax-Element-9">d</script> of the LDS models; and (3) the complicated temporal dependence inherent to time-series data. To tackle these challenges, we develop a two-stage meta-algorithm, which is guaranteed to efficiently recover each ground-truth LDS model up to error <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-10-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>O</mi><mo stretchy=&quot;false&quot;>&amp;#x007E;</mo></mover></mrow><mo stretchy=&quot;false&quot;>(</mo><msqrt><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>/</mo></mrow><mi>T</mi></msqrt><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation"><span id="MJXc-Node-72" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-73" class="mjx-mrow"><span id="MJXc-Node-74" class="mjx-texatom"><span id="MJXc-Node-75" class="mjx-mrow"><span id="MJXc-Node-76" class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.153em; padding-bottom: 0.06em; padding-left: 0.215em;"><span id="MJXc-Node-78" class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.003em; padding-bottom: 0.316em;">~</span></span></span><span class="mjx-op"><span id="MJXc-Node-77" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.503em; padding-bottom: 0.316em;">O</span></span></span></span></span></span></span><span id="MJXc-Node-79" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.441em; padding-bottom: 0.566em;">(</span></span><span id="MJXc-Node-80" class="mjx-msqrt"><span class="mjx-box" style="padding-top: 0.045em;"><span class="mjx-surd"><span class="mjx-char MJXc-TeX-size1-R" style="padding-top: 0.628em; padding-bottom: 0.628em;">√</span></span><span class="mjx-box" style="padding-top: 0.063em; border-top: 1.2px solid;"><span id="MJXc-Node-81" class="mjx-mrow"><span id="MJXc-Node-82" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.503em; padding-bottom: 0.316em; padding-right: 0.003em;">d</span></span><span id="MJXc-Node-83" class="mjx-texatom"><span id="MJXc-Node-84" class="mjx-mrow"><span id="MJXc-Node-85" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.441em; padding-bottom: 0.566em;">/</span></span></span></span><span id="MJXc-Node-86" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.441em; padding-bottom: 0.253em; padding-right: 0.12em;">T</span></span></span></span></span></span><span id="MJXc-Node-87" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.441em; padding-bottom: 0.566em;">)</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mover><mi>O</mi><mo stretchy="false">~</mo></mover></mrow><mo stretchy="false">(</mo><msqrt><mi>d</mi><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mi>T</mi></msqrt><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-10">\tilde{O}(\sqrt{d/T})</script>, where <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-11-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>T</mi></math>" role="presentation"><span id="MJXc-Node-88" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-89" class="mjx-mrow"><span id="MJXc-Node-90" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.441em; padding-bottom: 0.253em; padding-right: 0.12em;">T</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi></math></span></span><script type="math/tex" id="MathJax-Element-11">T</script> is the total sample size. We validate our theoretical studies with numerical experiments, confirming the efficacy of the proposed algorithm.
</div>
</div>

</div>
</td>

</tr>

<tr>
<td>

<div>Outstanding Paper</div>

</td>
<td>

<div class="displaycards touchup-date" id="event-17121">




<div style="width:80%;margin:auto;">
<a class="small-title" href="/virtual/2022/poster/17121">Causal Conceptions of Fairness and their Consequences</a>
</div>
<div class="type_display_name_minus_type"></div>
<div class="author-str">Hamed Nilforoshan · Johann Gaebler · Ravi Shroff · Sharad Goel</div>
<div class="author-str higher"></div>
<div class="text-muted touchup-date-div" id="touchup-date-event-17121">Thu 21 Jul 10:00 PM UTC</div>


<p style="font-size:.9em;">[ Hall E ]</p>






<div class="abstract-section">

<div>
<a id="abstract-link-17121" class="abstract-link" data-toggle="collapse" href="#collapse-event-abstract-17121" role="button" aria-expanded="false" aria-controls="collapse-event-abstract-17121">
Abstract <i id="caret-17121" class="fas fa-caret-right" aria-hidden="true"></i>
</a>
</div>


</div>
<div class="collapse" id="collapse-event-abstract-17121">
<div class="abstract-display">
<p>Recent work highlights the role of causality in designing equitable decision-making algorithms. It is not immediately clear, however, how existing causal conceptions of fairness relate to one another, or what the consequences are of using these definitions as design principles. Here, we first assemble and categorize popular causal definitions of algorithmic fairness into two broad families: (1) those that constrain the effects of decisions on counterfactual disparities; and (2) those that constrain the effects of legally protected characteristics, like race and gender, on decisions. We then show, analytically and empirically, that both families of definitions \emph{almost always}---in a measure theoretic sense---result in strongly Pareto dominated decision policies, meaning there is an alternative, unconstrained policy favored by every stakeholder with preferences drawn from a large, natural class. For example, in the case of college admissions decisions, policies constrained to satisfy causal fairness definitions would be disfavored by every stakeholder with neutral or positive preferences for both academic preparedness and diversity.Indeed, under a prominent definition of causal fairness, we prove the resulting policies require admitting all students with the same probability, regardless of academic qualifications or group membership. Our results highlight formal limitations and potential adverse consequences of common mathematical notions of …</p>
</div>
</div>

</div>
</td>

</tr>

<tr>
<td>

<div>Outstanding Paper</div>

</td>
<td>

<div class="displaycards touchup-date" id="event-15994">




<div style="width:80%;margin:auto;">
<a class="small-title" href="/virtual/2022/oral/15994">Learning Mixtures of Linear Dynamical Systems</a>
</div>
<div class="type_display_name_minus_type"></div>
<div class="author-str">Yanxi Chen · H. Vincent Poor</div>
<div class="author-str higher"></div>
<div class="text-muted touchup-date-div" id="touchup-date-event-15994">Tue 19 Jul 05:30 PM UTC</div>


<p style="font-size:.9em;">[ Hall G ]</p>






<div class="abstract-section">

<div>
<a id="abstract-link-15994" class="abstract-link" data-toggle="collapse" href="#collapse-event-abstract-15994" role="button" aria-expanded="false" aria-controls="collapse-event-abstract-15994">
Abstract <i id="caret-15994" class="fas fa-caret-right" aria-hidden="true"></i>
</a>
</div>


</div>
<div class="collapse" id="collapse-event-abstract-15994">
<div class="abstract-display">
We study the problem of learning a mixture of multiple linear dynamical systems (LDSs) from unlabeled short sample trajectories, each generated by one of the LDS models. Despite the wide applicability of mixture models for time-series data, learning algorithms that come with end-to-end performance guarantees are largely absent from existing literature. There are multiple sources of technical challenges, including but not limited to (1) the presence of latent variables (i.e. the unknown labels of trajectories); (2) the possibility that the sample trajectories might have lengths much smaller than the dimension <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-12-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi></math>" role="presentation"><span id="MJXc-Node-91" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-92" class="mjx-mrow"><span id="MJXc-Node-93" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.503em; padding-bottom: 0.316em; padding-right: 0.003em;">d</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>d</mi></math></span></span><script type="math/tex" id="MathJax-Element-12">d</script> of the LDS models; and (3) the complicated temporal dependence inherent to time-series data. To tackle these challenges, we develop a two-stage meta-algorithm, which is guaranteed to efficiently recover each ground-truth LDS model up to error <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-13-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>O</mi><mo stretchy=&quot;false&quot;>&amp;#x007E;</mo></mover></mrow><mo stretchy=&quot;false&quot;>(</mo><msqrt><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>/</mo></mrow><mi>T</mi></msqrt><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation"><span id="MJXc-Node-94" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-95" class="mjx-mrow"><span id="MJXc-Node-96" class="mjx-texatom"><span id="MJXc-Node-97" class="mjx-mrow"><span id="MJXc-Node-98" class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.153em; padding-bottom: 0.06em; padding-left: 0.215em;"><span id="MJXc-Node-100" class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.003em; padding-bottom: 0.316em;">~</span></span></span><span class="mjx-op"><span id="MJXc-Node-99" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.503em; padding-bottom: 0.316em;">O</span></span></span></span></span></span></span><span id="MJXc-Node-101" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.441em; padding-bottom: 0.566em;">(</span></span><span id="MJXc-Node-102" class="mjx-msqrt"><span class="mjx-box" style="padding-top: 0.045em;"><span class="mjx-surd"><span class="mjx-char MJXc-TeX-size1-R" style="padding-top: 0.628em; padding-bottom: 0.628em;">√</span></span><span class="mjx-box" style="padding-top: 0.063em; border-top: 1.2px solid;"><span id="MJXc-Node-103" class="mjx-mrow"><span id="MJXc-Node-104" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.503em; padding-bottom: 0.316em; padding-right: 0.003em;">d</span></span><span id="MJXc-Node-105" class="mjx-texatom"><span id="MJXc-Node-106" class="mjx-mrow"><span id="MJXc-Node-107" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.441em; padding-bottom: 0.566em;">/</span></span></span></span><span id="MJXc-Node-108" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.441em; padding-bottom: 0.253em; padding-right: 0.12em;">T</span></span></span></span></span></span><span id="MJXc-Node-109" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.441em; padding-bottom: 0.566em;">)</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mover><mi>O</mi><mo stretchy="false">~</mo></mover></mrow><mo stretchy="false">(</mo><msqrt><mi>d</mi><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mi>T</mi></msqrt><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-13">\tilde{O}(\sqrt{d/T})</script>, where <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-14-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>T</mi></math>" role="presentation"><span id="MJXc-Node-110" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-111" class="mjx-mrow"><span id="MJXc-Node-112" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.441em; padding-bottom: 0.253em; padding-right: 0.12em;">T</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi></math></span></span><script type="math/tex" id="MathJax-Element-14">T</script> is the total sample size. We validate our theoretical studies with numerical experiments, confirming the efficacy of the proposed algorithm.
</div>
</div>

</div>
</td>

</tr>

<tr>
<td>

<div>Outstanding Paper</div>

</td>
<td>

<div class="displaycards touchup-date" id="event-16289">




<div style="width:80%;margin:auto;">
<a class="small-title" href="/virtual/2022/poster/16289">The Importance of Non-Markovianity in Maximum State Entropy Exploration</a>
</div>
<div class="type_display_name_minus_type"></div>
<div class="author-str">Mirco Mutti · Riccardo De Santi · Marcello Restelli</div>
<div class="author-str higher"></div>
<div class="text-muted touchup-date-div" id="touchup-date-event-16289">Thu 21 Jul 10:00 PM UTC</div>


<p style="font-size:.9em;">[ Hall E ]</p>






<div class="abstract-section">

<div>
<a id="abstract-link-16289" class="abstract-link" data-toggle="collapse" href="#collapse-event-abstract-16289" role="button" aria-expanded="false" aria-controls="collapse-event-abstract-16289">
Abstract <i id="caret-16289" class="fas fa-caret-right" aria-hidden="true"></i>
</a>
</div>


</div>
<div class="collapse" id="collapse-event-abstract-16289">
<div class="abstract-display">
<p>In the maximum state entropy exploration framework, an agent interacts with a reward-free environment to learn a policy that maximizes the entropy of the expected state visitations it is inducing. Hazan et al. (2019) noted that the class of Markovian stochastic policies is sufficient for the maximum state entropy objective, and exploiting non-Markovianity is generally considered pointless in this setting. In this paper, we argue that non-Markovianity is instead paramount for maximum state entropy exploration in a finite-sample regime. Especially, we recast the objective to target the expected entropy of the induced state visitations in a single trial. Then, we show that the class of non-Markovian deterministic policies is sufficient for the introduced objective, while Markovian policies suffer non-zero regret in general. However, we prove that the problem of finding an optimal non-Markovian policy is NP-hard. Despite this negative result, we discuss avenues to address the problem in a tractable way and how non-Markovian exploration could benefit the sample efficiency of online reinforcement learning in future works.</p>

</div>
</div>

</div>
</td>

</tr>

<tr>
<td>

<div>Outstanding Paper</div>

</td>
<td>

<div class="displaycards touchup-date" id="event-17692">




<div style="width:80%;margin:auto;">
<a class="small-title" href="/virtual/2022/oral/17692">Solving Stackelberg Prediction Game with Least Squares Loss via Spherically Constrained Least Squares Reformulation</a>
</div>
<div class="type_display_name_minus_type"></div>
<div class="author-str">Jiali Wang · Wen Huang · Rujun Jiang · Xudong Li · Alex Wang</div>
<div class="author-str higher"></div>
<div class="text-muted touchup-date-div" id="touchup-date-event-17692">Tue 19 Jul 06:10 PM UTC</div>


<p style="font-size:.9em;">[ Room 318 - 320 ]</p>






<div class="abstract-section">

<div>
<a id="abstract-link-17692" class="abstract-link" data-toggle="collapse" href="#collapse-event-abstract-17692" role="button" aria-expanded="false" aria-controls="collapse-event-abstract-17692">
Abstract <i id="caret-17692" class="fas fa-caret-right" aria-hidden="true"></i>
</a>
</div>


</div>
<div class="collapse" id="collapse-event-abstract-17692">
<div class="abstract-display">
The Stackelberg prediction game (SPG) is popular in characterizing strategic interactions between a learner and an attacker. As an important special case, the SPG with least squares loss (SPG-LS) has recently received much research attention. Although initially formulated as a difficult bi-level optimization problem, SPG-LS admits tractable reformulations which can be polynomially globally solved by semidefinite programming or second order cone programming. However, all the available approaches are not well-suited for handling large-scale datasets, especially those with huge numbers of features.  In this paper, we explore an alternative reformulation of the SPG-LS. By a novel nonlinear change of variables, we rewrite the SPG-LS  as a spherically constrained least squares (SCLS) problem. Theoretically, we show that an <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-15-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03F5;</mi></math>" role="presentation"><span id="MJXc-Node-113" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-114" class="mjx-mrow"><span id="MJXc-Node-115" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">ϵ</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>ϵ</mi></math></span></span><script type="math/tex" id="MathJax-Element-15">\epsilon</script> optimal solutions to the SCLS (and the SPG-LS) can be achieved in <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-16-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>O</mi><mo stretchy=&quot;false&quot;>&amp;#x007E;</mo></mover></mrow><mo stretchy=&quot;false&quot;>(</mo><mi>N</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>/</mo></mrow><msqrt><mi>&amp;#x03F5;</mi></msqrt><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation"><span id="MJXc-Node-116" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-117" class="mjx-mrow"><span id="MJXc-Node-118" class="mjx-texatom"><span id="MJXc-Node-119" class="mjx-mrow"><span id="MJXc-Node-120" class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.153em; padding-bottom: 0.06em; padding-left: 0.215em;"><span id="MJXc-Node-122" class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.003em; padding-bottom: 0.316em;">~</span></span></span><span class="mjx-op"><span id="MJXc-Node-121" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.503em; padding-bottom: 0.316em;">O</span></span></span></span></span></span></span><span id="MJXc-Node-123" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.441em; padding-bottom: 0.566em;">(</span></span><span id="MJXc-Node-124" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.441em; padding-bottom: 0.253em; padding-right: 0.085em;">N</span></span><span id="MJXc-Node-125" class="mjx-texatom"><span id="MJXc-Node-126" class="mjx-mrow"><span id="MJXc-Node-127" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.441em; padding-bottom: 0.566em;">/</span></span></span></span><span id="MJXc-Node-128" class="mjx-msqrt"><span class="mjx-box" style="padding-top: 0.045em;"><span class="mjx-surd"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.503em; padding-bottom: 0.566em;">√</span></span><span class="mjx-box" style="padding-top: 0.242em; border-top: 1.2px solid;"><span id="MJXc-Node-129" class="mjx-mrow"><span id="MJXc-Node-130" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">ϵ</span></span></span></span></span></span><span id="MJXc-Node-131" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.441em; padding-bottom: 0.566em;">)</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mover><mi>O</mi><mo stretchy="false">~</mo></mover></mrow><mo stretchy="false">(</mo><mi>N</mi><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><msqrt><mi>ϵ</mi></msqrt><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-16">\tilde O(N/\sqrt{\epsilon})</script> floating-point operations, where <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-17-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi></math>" role="presentation"><span id="MJXc-Node-132" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-133" class="mjx-mrow"><span id="MJXc-Node-134" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.441em; padding-bottom: 0.253em; padding-right: 0.085em;">N</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi></math></span></span><script type="math/tex" id="MathJax-Element-17">N</script> is the number of nonzero entries in the data matrix. Practically, we apply two well-known methods for solving this new reformulation, i.e., the Krylov subspace method and the Riemannian trust region method. Both algorithms are factorization free so that they are suitable for solving large scale problems. Numerical results on both synthetic and real-world datasets indicate that the SPG-LS, equipped with the SCLS reformulation, can …
</div>
</div>

</div>
</td>

</tr>

<tr>
<td>

<div>Outstanding Paper</div>

</td>
<td>

<div class="displaycards touchup-date" id="event-18235">




<div style="width:80%;margin:auto;">
<a class="small-title" href="/virtual/2022/poster/18235">Privacy for Free: How does Dataset Condensation Help Privacy?</a>
</div>
<div class="type_display_name_minus_type"></div>
<div class="author-str">Tian Dong · Bo Zhao · Lingjuan Lyu</div>
<div class="author-str higher"></div>
<div class="text-muted touchup-date-div" id="touchup-date-event-18235">Wed 20 Jul 10:30 PM UTC</div>


<p style="font-size:.9em;">[ Hall E ]</p>






<div class="abstract-section">

<div>
<a id="abstract-link-18235" class="abstract-link" data-toggle="collapse" href="#collapse-event-abstract-18235" role="button" aria-expanded="false" aria-controls="collapse-event-abstract-18235">
Abstract <i id="caret-18235" class="fas fa-caret-right" aria-hidden="true"></i>
</a>
</div>


</div>
<div class="collapse" id="collapse-event-abstract-18235">
<div class="abstract-display">
To prevent unintentional data leakage, research community has resorted to data generators that can produce differentially private data for model training. However, for the sake of the data privacy, existing solutions suffer from either expensive training cost or poor generalization performance. Therefore, we raise the question whether training efficiency and privacy can be achieved simultaneously. In this work, we for the first time identify that dataset condensation (DC) which is originally designed for improving training efficiency is also a better solution to replace the traditional data generators for private data generation, thus providing privacy for free. To demonstrate the privacy benefit of DC, we build a connection between DC and differential privacy, and theoretically prove on linear feature extractors (and then extended to non-linear feature extractors) that the existence of one sample has limited impact (<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-18-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mi>m</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>/</mo></mrow><mi>n</mi><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation"><span id="MJXc-Node-135" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-136" class="mjx-mrow"><span id="MJXc-Node-137" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.503em; padding-bottom: 0.316em;">O</span></span><span id="MJXc-Node-138" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.441em; padding-bottom: 0.566em;">(</span></span><span id="MJXc-Node-139" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">m</span></span><span id="MJXc-Node-140" class="mjx-texatom"><span id="MJXc-Node-141" class="mjx-mrow"><span id="MJXc-Node-142" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.441em; padding-bottom: 0.566em;">/</span></span></span></span><span id="MJXc-Node-143" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">n</span></span><span id="MJXc-Node-144" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.441em; padding-bottom: 0.566em;">)</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><mi>m</mi><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mi>n</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-18">O(m/n)</script>) on the parameter distribution of networks trained on <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-19-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>m</mi></math>" role="presentation"><span id="MJXc-Node-145" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-146" class="mjx-mrow"><span id="MJXc-Node-147" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">m</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>m</mi></math></span></span><script type="math/tex" id="MathJax-Element-19">m</script> samples synthesized from <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-20-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi><mo stretchy=&quot;false&quot;>(</mo><mi>n</mi><mo>&amp;#x226B;</mo><mi>m</mi><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation"><span id="MJXc-Node-148" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-149" class="mjx-mrow"><span id="MJXc-Node-150" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">n</span></span><span id="MJXc-Node-151" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.441em; padding-bottom: 0.566em;">(</span></span><span id="MJXc-Node-152" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">n</span></span><span id="MJXc-Node-153" class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.253em; padding-bottom: 0.378em;">≫</span></span><span id="MJXc-Node-154" class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">m</span></span><span id="MJXc-Node-155" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.441em; padding-bottom: 0.566em;">)</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi><mo stretchy="false">(</mo><mi>n</mi><mo>≫</mo><mi>m</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-20">n (n \gg m)</script> raw samples by DC. We also empirically validate the visual privacy and membership privacy of DC-synthesized data by launching both the loss-based and the state-of-the-art likelihood-based membership inference attacks. We envision this work as a milestone for data-efficient and privacy-preserving machine learning.
</div>
</div>

</div>
</td>

</tr>

<tr>
<td>

<div>Outstanding Paper</div>

</td>
<td>

<div class="displaycards touchup-date" id="event-16665">




<div style="width:80%;margin:auto;">
<a class="small-title" href="/virtual/2022/poster/16665">G-Mixup: Graph Data Augmentation for Graph Classification</a>
</div>
<div class="type_display_name_minus_type"></div>
<div class="author-str">Xiaotian Han · Zhimeng Jiang · Ninghao Liu · Xia Hu</div>
<div class="author-str higher"></div>
<div class="text-muted touchup-date-div" id="touchup-date-event-16665">Thu 21 Jul 10:00 PM UTC</div>


<p style="font-size:.9em;">[ Hall E ]</p>






<div class="abstract-section">

<div>
<a id="abstract-link-16665" class="abstract-link" data-toggle="collapse" href="#collapse-event-abstract-16665" role="button" aria-expanded="false" aria-controls="collapse-event-abstract-16665">
Abstract <i id="caret-16665" class="fas fa-caret-right" aria-hidden="true"></i>
</a>
</div>


</div>
<div class="collapse" id="collapse-event-abstract-16665">
<div class="abstract-display">
<p>This work develops mixup for graph data. Mixup has shown superiority in improving the generalization and robustness of neural networks by interpolating features and labels between two random samples. Traditionally, Mixup can work on regular, grid-like, and Euclidean data such as image or tabular data. However, it is challenging to directly adopt Mixup to augment graph data because different graphs typically: 1) have different numbers of nodes; 2) are not readily aligned; and 3) have unique typologies in non-Euclidean space. To this end, we propose G-Mixup to augment graphs for graph classification by interpolating the generator (i.e., graphon) of different classes of graphs. Specifically, we first use graphs within the same class to estimate a graphon. Then, instead of directly manipulating graphs, we interpolate graphons of different classes in the Euclidean space to get mixed graphons, where the synthetic graphs are generated through sampling based on the mixed graphons. Extensive experiments show that G-Mixup substantially improves the generalization and robustness of GNNs.</p>

</div>
</div>

</div>
</td>

</tr>

<tr>
<td>

<div>Outstanding Paper</div>

</td>
<td>

<div class="displaycards touchup-date" id="event-16634">




<div style="width:80%;margin:auto;">
<a class="small-title" href="/virtual/2022/oral/16634">Understanding Dataset Difficulty with <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-21-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 120%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>V</mi></mrow></math>" role="presentation"><span id="MJXc-Node-156" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-157" class="mjx-mrow"><span id="MJXc-Node-158" class="mjx-texatom"><span id="MJXc-Node-159" class="mjx-mrow"><span id="MJXc-Node-160" class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.441em; padding-bottom: 0.378em; padding-right: 0.045em;">V</span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">V</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-21">\mathcal{V}</script>-Usable Information</a>
</div>
<div class="type_display_name_minus_type"></div>
<div class="author-str">Kawin Ethayarajh · Yejin Choi · Swabha Swayamdipta</div>
<div class="author-str higher"></div>
<div class="text-muted touchup-date-div" id="touchup-date-event-16634">Tue 19 Jul 03:00 PM UTC</div>


<p style="font-size:.9em;">[ Room 301 - 303 ]</p>






<div class="abstract-section">

<div>
<a id="abstract-link-16634" class="abstract-link" data-toggle="collapse" href="#collapse-event-abstract-16634" role="button" aria-expanded="false" aria-controls="collapse-event-abstract-16634">
Abstract <i id="caret-16634" class="fas fa-caret-right" aria-hidden="true"></i>
</a>
</div>


</div>
<div class="collapse" id="collapse-event-abstract-16634">
<div class="abstract-display">
Estimating the difficulty of a dataset typically involves comparing state-of-the-art models to humans; the bigger the performance gap, the harder the dataset is said to be. However, this comparison provides little understanding of how difficult each instance in a given distribution is, or what attributes make the dataset difficult for a given model. To address these questions, we frame dataset difficulty---w.r.t. a model <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-22-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>V</mi></mrow></math>" role="presentation"><span id="MJXc-Node-161" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-162" class="mjx-mrow"><span id="MJXc-Node-163" class="mjx-texatom"><span id="MJXc-Node-164" class="mjx-mrow"><span id="MJXc-Node-165" class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.441em; padding-bottom: 0.378em; padding-right: 0.045em;">V</span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">V</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-22">\mathcal{V}</script>---as the lack of <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-23-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>V</mi></mrow></math>" role="presentation"><span id="MJXc-Node-166" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-167" class="mjx-mrow"><span id="MJXc-Node-168" class="mjx-texatom"><span id="MJXc-Node-169" class="mjx-mrow"><span id="MJXc-Node-170" class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.441em; padding-bottom: 0.378em; padding-right: 0.045em;">V</span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">V</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-23">\mathcal{V}</script>-usable information (Xu et al., 2019), where a lower value indicates a more difficult dataset for <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-24-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>V</mi></mrow></math>" role="presentation"><span id="MJXc-Node-171" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-172" class="mjx-mrow"><span id="MJXc-Node-173" class="mjx-texatom"><span id="MJXc-Node-174" class="mjx-mrow"><span id="MJXc-Node-175" class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.441em; padding-bottom: 0.378em; padding-right: 0.045em;">V</span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">V</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-24">\mathcal{V}</script>. We further introduce pointwise <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-25-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>V</mi></mrow></math>" role="presentation"><span id="MJXc-Node-176" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-177" class="mjx-mrow"><span id="MJXc-Node-178" class="mjx-texatom"><span id="MJXc-Node-179" class="mjx-mrow"><span id="MJXc-Node-180" class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.441em; padding-bottom: 0.378em; padding-right: 0.045em;">V</span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">V</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-25">\mathcal{V}</script>-information (PVI) for measuring the difficulty of individual instances w.r.t. a given distribution. While standard evaluation metrics typically only compare different models for the same dataset, <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-26-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>V</mi></mrow></math>" role="presentation"><span id="MJXc-Node-181" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-182" class="mjx-mrow"><span id="MJXc-Node-183" class="mjx-texatom"><span id="MJXc-Node-184" class="mjx-mrow"><span id="MJXc-Node-185" class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.441em; padding-bottom: 0.378em; padding-right: 0.045em;">V</span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">V</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-26">\mathcal{V}</script>-usable information and PVI also permit the converse: for a given model <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-27-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>V</mi></mrow></math>" role="presentation"><span id="MJXc-Node-186" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-187" class="mjx-mrow"><span id="MJXc-Node-188" class="mjx-texatom"><span id="MJXc-Node-189" class="mjx-mrow"><span id="MJXc-Node-190" class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.441em; padding-bottom: 0.378em; padding-right: 0.045em;">V</span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">V</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-27">\mathcal{V}</script>, we can compare different datasets, as well as different instances/slices of the same dataset. Furthermore, our framework allows for the interpretability of different input attributes via transformations of the input, which we use to discover annotation artefacts in widely-used NLP benchmarks.
</div>
</div>

</div>
</td>

</tr>

<tr>
<td>

<div>Outstanding Paper</div>

</td>
<td>

<div class="displaycards touchup-date" id="event-16633">




<div style="width:80%;margin:auto;">
<a class="small-title" href="/virtual/2022/poster/16633">Understanding Dataset Difficulty with <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-28-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 120%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>V</mi></mrow></math>" role="presentation"><span id="MJXc-Node-191" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-192" class="mjx-mrow"><span id="MJXc-Node-193" class="mjx-texatom"><span id="MJXc-Node-194" class="mjx-mrow"><span id="MJXc-Node-195" class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.441em; padding-bottom: 0.378em; padding-right: 0.045em;">V</span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">V</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-28">\mathcal{V}</script>-Usable Information</a>
</div>
<div class="type_display_name_minus_type"></div>
<div class="author-str">Kawin Ethayarajh · Yejin Choi · Swabha Swayamdipta</div>
<div class="author-str higher"></div>
<div class="text-muted touchup-date-div" id="touchup-date-event-16633">Tue 19 Jul 10:30 PM UTC</div>


<p style="font-size:.9em;">[ Hall E ]</p>






<div class="abstract-section">

<div>
<a id="abstract-link-16633" class="abstract-link" data-toggle="collapse" href="#collapse-event-abstract-16633" role="button" aria-expanded="false" aria-controls="collapse-event-abstract-16633">
Abstract <i id="caret-16633" class="fas fa-caret-right" aria-hidden="true"></i>
</a>
</div>


</div>
<div class="collapse" id="collapse-event-abstract-16633">
<div class="abstract-display">
Estimating the difficulty of a dataset typically involves comparing state-of-the-art models to humans; the bigger the performance gap, the harder the dataset is said to be. However, this comparison provides little understanding of how difficult each instance in a given distribution is, or what attributes make the dataset difficult for a given model. To address these questions, we frame dataset difficulty---w.r.t. a model <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-29-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>V</mi></mrow></math>" role="presentation"><span id="MJXc-Node-196" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-197" class="mjx-mrow"><span id="MJXc-Node-198" class="mjx-texatom"><span id="MJXc-Node-199" class="mjx-mrow"><span id="MJXc-Node-200" class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.441em; padding-bottom: 0.378em; padding-right: 0.045em;">V</span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">V</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-29">\mathcal{V}</script>---as the lack of <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-30-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>V</mi></mrow></math>" role="presentation"><span id="MJXc-Node-201" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-202" class="mjx-mrow"><span id="MJXc-Node-203" class="mjx-texatom"><span id="MJXc-Node-204" class="mjx-mrow"><span id="MJXc-Node-205" class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.441em; padding-bottom: 0.378em; padding-right: 0.045em;">V</span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">V</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-30">\mathcal{V}</script>-usable information (Xu et al., 2019), where a lower value indicates a more difficult dataset for <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-31-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>V</mi></mrow></math>" role="presentation"><span id="MJXc-Node-206" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-207" class="mjx-mrow"><span id="MJXc-Node-208" class="mjx-texatom"><span id="MJXc-Node-209" class="mjx-mrow"><span id="MJXc-Node-210" class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.441em; padding-bottom: 0.378em; padding-right: 0.045em;">V</span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">V</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-31">\mathcal{V}</script>. We further introduce pointwise <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-32-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>V</mi></mrow></math>" role="presentation"><span id="MJXc-Node-211" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-212" class="mjx-mrow"><span id="MJXc-Node-213" class="mjx-texatom"><span id="MJXc-Node-214" class="mjx-mrow"><span id="MJXc-Node-215" class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.441em; padding-bottom: 0.378em; padding-right: 0.045em;">V</span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">V</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-32">\mathcal{V}</script>-information (PVI) for measuring the difficulty of individual instances w.r.t. a given distribution. While standard evaluation metrics typically only compare different models for the same dataset, <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-33-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>V</mi></mrow></math>" role="presentation"><span id="MJXc-Node-216" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-217" class="mjx-mrow"><span id="MJXc-Node-218" class="mjx-texatom"><span id="MJXc-Node-219" class="mjx-mrow"><span id="MJXc-Node-220" class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.441em; padding-bottom: 0.378em; padding-right: 0.045em;">V</span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">V</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-33">\mathcal{V}</script>-usable information and PVI also permit the converse: for a given model <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-34-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>V</mi></mrow></math>" role="presentation"><span id="MJXc-Node-221" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-222" class="mjx-mrow"><span id="MJXc-Node-223" class="mjx-texatom"><span id="MJXc-Node-224" class="mjx-mrow"><span id="MJXc-Node-225" class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.441em; padding-bottom: 0.378em; padding-right: 0.045em;">V</span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">V</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-34">\mathcal{V}</script>, we can compare different datasets, as well as different instances/slices of the same dataset. Furthermore, our framework allows for the interpretability of different input attributes via transformations of the input, which we use to discover annotation artefacts in widely-used NLP benchmarks.
</div>
</div>

</div>
</td>

</tr>

<tr>
<td>

<div>Outstanding Paper</div>

</td>
<td>

<div class="displaycards touchup-date" id="event-16841">




<div style="width:80%;margin:auto;">
<a class="small-title" href="/virtual/2022/poster/16841">Stable Conformal Prediction Sets</a>
</div>
<div class="type_display_name_minus_type"></div>
<div class="author-str">Eugene Ndiaye</div>
<div class="author-str higher"></div>
<div class="text-muted touchup-date-div" id="touchup-date-event-16841">Wed 20 Jul 10:30 PM UTC</div>


<p style="font-size:.9em;">[ Hall E ]</p>






<div class="abstract-section">

<div>
<a id="abstract-link-16841" class="abstract-link" data-toggle="collapse" href="#collapse-event-abstract-16841" role="button" aria-expanded="false" aria-controls="collapse-event-abstract-16841">
Abstract <i id="caret-16841" class="fas fa-caret-right" aria-hidden="true"></i>
</a>
</div>


</div>
<div class="collapse" id="collapse-event-abstract-16841">
<div class="abstract-display">
When one observes a sequence of variables <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-35-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>y</mi><mn>1</mn></msub><mo stretchy=&quot;false&quot;>)</mo><mo>,</mo><mo>&amp;#x2026;</mo><mo>,</mo><mo stretchy=&quot;false&quot;>(</mo><msub><mi>x</mi><mi>n</mi></msub><mo>,</mo><msub><mi>y</mi><mi>n</mi></msub><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation"><span id="MJXc-Node-226" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-227" class="mjx-mrow"><span id="MJXc-Node-228" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.441em; padding-bottom: 0.566em;">(</span></span><span id="MJXc-Node-229" class="mjx-msubsup"><span class="mjx-base"><span id="MJXc-Node-230" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">x</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span id="MJXc-Node-231" class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.378em; padding-bottom: 0.316em;">1</span></span></span></span><span id="MJXc-Node-232" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.184em; padding-bottom: 0.566em;">,</span></span><span id="MJXc-Node-233" class="mjx-msubsup MJXc-space1"><span class="mjx-base" style="margin-right: -0.006em;"><span id="MJXc-Node-234" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.503em; padding-right: 0.006em;">y</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span id="MJXc-Node-235" class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.378em; padding-bottom: 0.316em;">1</span></span></span></span><span id="MJXc-Node-236" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.441em; padding-bottom: 0.566em;">)</span></span><span id="MJXc-Node-237" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.184em; padding-bottom: 0.566em;">,</span></span><span id="MJXc-Node-238" class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.184em; padding-bottom: 0.316em;">…</span></span><span id="MJXc-Node-239" class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.184em; padding-bottom: 0.566em;">,</span></span><span id="MJXc-Node-240" class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.441em; padding-bottom: 0.566em;">(</span></span><span id="MJXc-Node-241" class="mjx-msubsup"><span class="mjx-base"><span id="MJXc-Node-242" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">x</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span id="MJXc-Node-243" class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">n</span></span></span></span><span id="MJXc-Node-244" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.184em; padding-bottom: 0.566em;">,</span></span><span id="MJXc-Node-245" class="mjx-msubsup MJXc-space1"><span class="mjx-base" style="margin-right: -0.006em;"><span id="MJXc-Node-246" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.503em; padding-right: 0.006em;">y</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span id="MJXc-Node-247" class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">n</span></span></span></span><span id="MJXc-Node-248" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.441em; padding-bottom: 0.566em;">)</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>y</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mo>,</mo><mo>…</mo><mo>,</mo><mo stretchy="false">(</mo><msub><mi>x</mi><mi>n</mi></msub><mo>,</mo><msub><mi>y</mi><mi>n</mi></msub><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-35">(x_1, y_1), \ldots, (x_n, y_n)</script>, Conformal Prediction (CP) is a methodology that allows to estimate a confidence set for <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-36-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub></math>" role="presentation"><span id="MJXc-Node-249" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-250" class="mjx-mrow"><span id="MJXc-Node-251" class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.006em;"><span id="MJXc-Node-252" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.503em; padding-right: 0.006em;">y</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span id="MJXc-Node-253" class="mjx-texatom" style=""><span id="MJXc-Node-254" class="mjx-mrow"><span id="MJXc-Node-255" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">n</span></span><span id="MJXc-Node-256" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.316em; padding-bottom: 0.441em;">+</span></span><span id="MJXc-Node-257" class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.378em; padding-bottom: 0.316em;">1</span></span></span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>y</mi><mrow class="MJX-TeXAtom-ORD"><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-36">y_{n+1}</script> given <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-37-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub></math>" role="presentation"><span id="MJXc-Node-258" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-259" class="mjx-mrow"><span id="MJXc-Node-260" class="mjx-msubsup"><span class="mjx-base"><span id="MJXc-Node-261" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">x</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span id="MJXc-Node-262" class="mjx-texatom" style=""><span id="MJXc-Node-263" class="mjx-mrow"><span id="MJXc-Node-264" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">n</span></span><span id="MJXc-Node-265" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.316em; padding-bottom: 0.441em;">+</span></span><span id="MJXc-Node-266" class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.378em; padding-bottom: 0.316em;">1</span></span></span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>x</mi><mrow class="MJX-TeXAtom-ORD"><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-37">x_{n+1}</script> by merely assuming that the distribution of the data is exchangeable. CP sets have guaranteed coverage for any finite population size <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-38-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi></math>" role="presentation"><span id="MJXc-Node-267" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-268" class="mjx-mrow"><span id="MJXc-Node-269" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">n</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></span></span><script type="math/tex" id="MathJax-Element-38">n</script>. While appealing, the computation of such a set turns out to be infeasible in general, \eg when the unknown variable <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-39-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub></math>" role="presentation"><span id="MJXc-Node-270" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-271" class="mjx-mrow"><span id="MJXc-Node-272" class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.006em;"><span id="MJXc-Node-273" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.503em; padding-right: 0.006em;">y</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span id="MJXc-Node-274" class="mjx-texatom" style=""><span id="MJXc-Node-275" class="mjx-mrow"><span id="MJXc-Node-276" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">n</span></span><span id="MJXc-Node-277" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.316em; padding-bottom: 0.441em;">+</span></span><span id="MJXc-Node-278" class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.378em; padding-bottom: 0.316em;">1</span></span></span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>y</mi><mrow class="MJX-TeXAtom-ORD"><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-39">y_{n+1}</script> is continuous. The bottleneck is that it is based on a procedure that readjusts a prediction model on data where we replace the unknown target by all its possible values in order to select the most probable one. This requires computing an infinite number of models, which often makes it intractable. In this paper, we combine CP techniques with classical algorithmic stability bounds to derive a prediction set computable with a single model fit. We demonstrate that our proposed confidence set does not lose any coverage guarantees while avoiding the need for data splitting as currently done in the literature. We provide some numerical experiments to illustrate the tightness of our estimation when the sample size is sufficiently large, on both synthetic and real datasets.
</div>
</div>

</div>
</td>

</tr>

<tr>
<td>

<div>Outstanding Paper</div>

</td>
<td>

<div class="displaycards touchup-date" id="event-17691">




<div style="width:80%;margin:auto;">
<a class="small-title" href="/virtual/2022/poster/17691">Solving Stackelberg Prediction Game with Least Squares Loss via Spherically Constrained Least Squares Reformulation</a>
</div>
<div class="type_display_name_minus_type"></div>
<div class="author-str">Jiali Wang · Wen Huang · Rujun Jiang · Xudong Li · Alex Wang</div>
<div class="author-str higher"></div>
<div class="text-muted touchup-date-div" id="touchup-date-event-17691">Tue 19 Jul 10:30 PM UTC</div>


<p style="font-size:.9em;">[ Hall E ]</p>






<div class="abstract-section">

<div>
<a id="abstract-link-17691" class="abstract-link" data-toggle="collapse" href="#collapse-event-abstract-17691" role="button" aria-expanded="false" aria-controls="collapse-event-abstract-17691">
Abstract <i id="caret-17691" class="fas fa-caret-right" aria-hidden="true"></i>
</a>
</div>


</div>
<div class="collapse" id="collapse-event-abstract-17691">
<div class="abstract-display">
The Stackelberg prediction game (SPG) is popular in characterizing strategic interactions between a learner and an attacker. As an important special case, the SPG with least squares loss (SPG-LS) has recently received much research attention. Although initially formulated as a difficult bi-level optimization problem, SPG-LS admits tractable reformulations which can be polynomially globally solved by semidefinite programming or second order cone programming. However, all the available approaches are not well-suited for handling large-scale datasets, especially those with huge numbers of features.  In this paper, we explore an alternative reformulation of the SPG-LS. By a novel nonlinear change of variables, we rewrite the SPG-LS  as a spherically constrained least squares (SCLS) problem. Theoretically, we show that an <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-40-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03F5;</mi></math>" role="presentation"><span id="MJXc-Node-279" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-280" class="mjx-mrow"><span id="MJXc-Node-281" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">ϵ</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>ϵ</mi></math></span></span><script type="math/tex" id="MathJax-Element-40">\epsilon</script> optimal solutions to the SCLS (and the SPG-LS) can be achieved in <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-41-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>O</mi><mo stretchy=&quot;false&quot;>&amp;#x007E;</mo></mover></mrow><mo stretchy=&quot;false&quot;>(</mo><mi>N</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>/</mo></mrow><msqrt><mi>&amp;#x03F5;</mi></msqrt><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation"><span id="MJXc-Node-282" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-283" class="mjx-mrow"><span id="MJXc-Node-284" class="mjx-texatom"><span id="MJXc-Node-285" class="mjx-mrow"><span id="MJXc-Node-286" class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.153em; padding-bottom: 0.06em; padding-left: 0.215em;"><span id="MJXc-Node-288" class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.003em; padding-bottom: 0.316em;">~</span></span></span><span class="mjx-op"><span id="MJXc-Node-287" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.503em; padding-bottom: 0.316em;">O</span></span></span></span></span></span></span><span id="MJXc-Node-289" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.441em; padding-bottom: 0.566em;">(</span></span><span id="MJXc-Node-290" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.441em; padding-bottom: 0.253em; padding-right: 0.085em;">N</span></span><span id="MJXc-Node-291" class="mjx-texatom"><span id="MJXc-Node-292" class="mjx-mrow"><span id="MJXc-Node-293" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.441em; padding-bottom: 0.566em;">/</span></span></span></span><span id="MJXc-Node-294" class="mjx-msqrt"><span class="mjx-box" style="padding-top: 0.045em;"><span class="mjx-surd"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.503em; padding-bottom: 0.566em;">√</span></span><span class="mjx-box" style="padding-top: 0.242em; border-top: 1.2px solid;"><span id="MJXc-Node-295" class="mjx-mrow"><span id="MJXc-Node-296" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">ϵ</span></span></span></span></span></span><span id="MJXc-Node-297" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.441em; padding-bottom: 0.566em;">)</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mover><mi>O</mi><mo stretchy="false">~</mo></mover></mrow><mo stretchy="false">(</mo><mi>N</mi><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><msqrt><mi>ϵ</mi></msqrt><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-41">\tilde O(N/\sqrt{\epsilon})</script> floating-point operations, where <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-42-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi></math>" role="presentation"><span id="MJXc-Node-298" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-299" class="mjx-mrow"><span id="MJXc-Node-300" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.441em; padding-bottom: 0.253em; padding-right: 0.085em;">N</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi></math></span></span><script type="math/tex" id="MathJax-Element-42">N</script> is the number of nonzero entries in the data matrix. Practically, we apply two well-known methods for solving this new reformulation, i.e., the Krylov subspace method and the Riemannian trust region method. Both algorithms are factorization free so that they are suitable for solving large scale problems. Numerical results on both synthetic and real-world datasets indicate that the SPG-LS, equipped with the SCLS reformulation, can …
</div>
</div>

</div>
</td>

</tr>

<tr>
<td>

<div>Outstanding Paper</div>

</td>
<td>

<div class="displaycards touchup-date" id="event-16770">




<div style="width:80%;margin:auto;">
<a class="small-title" href="/virtual/2022/oral/16770">Do Differentiable Simulators Give Better Policy Gradients?</a>
</div>
<div class="type_display_name_minus_type"></div>
<div class="author-str">Hyung Ju Suh · Max Simchowitz · Kaiqing Zhang · Russ Tedrake</div>
<div class="author-str higher"></div>
<div class="text-muted touchup-date-div" id="touchup-date-event-16770">Thu 21 Jul 06:05 PM UTC</div>


<p style="font-size:.9em;">[ Hall G ]</p>






<div class="abstract-section">

<div>
<a id="abstract-link-16770" class="abstract-link" data-toggle="collapse" href="#collapse-event-abstract-16770" role="button" aria-expanded="false" aria-controls="collapse-event-abstract-16770">
Abstract <i id="caret-16770" class="fas fa-caret-right" aria-hidden="true"></i>
</a>
</div>


</div>
<div class="collapse" id="collapse-event-abstract-16770">
<div class="abstract-display">
Differentiable simulators promise faster computation time for reinforcement learning by replacing zeroth-order gradient estimates of a stochastic objective with an estimate based on first-order gradients. However, it is yet unclear what factors decide the performance of the two estimators on complex landscapes that involve long-horizon planning and control on physical systems, despite the crucial relevance of this question for the utility of differentiable simulators. We show that characteristics of certain physical systems, such as stiffness or discontinuities, may compromise the efficacy of the first-order estimator, and analyze this phenomenon through the lens of bias and variance. We additionally propose an <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-43-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>" role="presentation"><span id="MJXc-Node-301" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-302" class="mjx-mrow"><span id="MJXc-Node-303" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">α</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>α</mi></math></span></span><script type="math/tex" id="MathJax-Element-43">\alpha</script>-order gradient estimator, with <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-44-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi><mo>&amp;#x2208;</mo><mo stretchy=&quot;false&quot;>[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=&quot;false&quot;>]</mo></math>" role="presentation"><span id="MJXc-Node-304" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-305" class="mjx-mrow"><span id="MJXc-Node-306" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">α</span></span><span id="MJXc-Node-307" class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.253em; padding-bottom: 0.378em;">∈</span></span><span id="MJXc-Node-308" class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.441em; padding-bottom: 0.566em;">[</span></span><span id="MJXc-Node-309" class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.378em; padding-bottom: 0.378em;">0</span></span><span id="MJXc-Node-310" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.184em; padding-bottom: 0.566em;">,</span></span><span id="MJXc-Node-311" class="mjx-mn MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.378em; padding-bottom: 0.316em;">1</span></span><span id="MJXc-Node-312" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.441em; padding-bottom: 0.566em;">]</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>α</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></math></span></span><script type="math/tex" id="MathJax-Element-44">\alpha \in [0,1]</script>, which correctly utilizes exact gradients to combine the efficiency of first-order estimates with the robustness of zero-order methods. We demonstrate the pitfalls of traditional estimators and the advantages of the <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-45-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>" role="presentation"><span id="MJXc-Node-313" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-314" class="mjx-mrow"><span id="MJXc-Node-315" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">α</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>α</mi></math></span></span><script type="math/tex" id="MathJax-Element-45">\alpha</script>-order estimator on some numerical examples.
</div>
</div>

</div>
</td>

</tr>

<tr>
<td>

<div>Outstanding Paper</div>

</td>
<td>

<div class="displaycards touchup-date" id="event-18236">




<div style="width:80%;margin:auto;">
<a class="small-title" href="/virtual/2022/oral/18236">Privacy for Free: How does Dataset Condensation Help Privacy?</a>
</div>
<div class="type_display_name_minus_type"></div>
<div class="author-str">Tian Dong · Bo Zhao · Lingjuan Lyu</div>
<div class="author-str higher"></div>
<div class="text-muted touchup-date-div" id="touchup-date-event-18236">Wed 20 Jul 03:05 PM UTC</div>


<p style="font-size:.9em;">[ Room 318 - 320 ]</p>






<div class="abstract-section">

<div>
<a id="abstract-link-18236" class="abstract-link" data-toggle="collapse" href="#collapse-event-abstract-18236" role="button" aria-expanded="false" aria-controls="collapse-event-abstract-18236">
Abstract <i id="caret-18236" class="fas fa-caret-right" aria-hidden="true"></i>
</a>
</div>


</div>
<div class="collapse" id="collapse-event-abstract-18236">
<div class="abstract-display">
To prevent unintentional data leakage, research community has resorted to data generators that can produce differentially private data for model training. However, for the sake of the data privacy, existing solutions suffer from either expensive training cost or poor generalization performance. Therefore, we raise the question whether training efficiency and privacy can be achieved simultaneously. In this work, we for the first time identify that dataset condensation (DC) which is originally designed for improving training efficiency is also a better solution to replace the traditional data generators for private data generation, thus providing privacy for free. To demonstrate the privacy benefit of DC, we build a connection between DC and differential privacy, and theoretically prove on linear feature extractors (and then extended to non-linear feature extractors) that the existence of one sample has limited impact (<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-46-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mi>m</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>/</mo></mrow><mi>n</mi><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation"><span id="MJXc-Node-316" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-317" class="mjx-mrow"><span id="MJXc-Node-318" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.503em; padding-bottom: 0.316em;">O</span></span><span id="MJXc-Node-319" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.441em; padding-bottom: 0.566em;">(</span></span><span id="MJXc-Node-320" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">m</span></span><span id="MJXc-Node-321" class="mjx-texatom"><span id="MJXc-Node-322" class="mjx-mrow"><span id="MJXc-Node-323" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.441em; padding-bottom: 0.566em;">/</span></span></span></span><span id="MJXc-Node-324" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">n</span></span><span id="MJXc-Node-325" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.441em; padding-bottom: 0.566em;">)</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><mi>m</mi><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mi>n</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-46">O(m/n)</script>) on the parameter distribution of networks trained on <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-47-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>m</mi></math>" role="presentation"><span id="MJXc-Node-326" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-327" class="mjx-mrow"><span id="MJXc-Node-328" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">m</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>m</mi></math></span></span><script type="math/tex" id="MathJax-Element-47">m</script> samples synthesized from <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-48-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 150%; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi><mo stretchy=&quot;false&quot;>(</mo><mi>n</mi><mo>&amp;#x226B;</mo><mi>m</mi><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation"><span id="MJXc-Node-329" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-330" class="mjx-mrow"><span id="MJXc-Node-331" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">n</span></span><span id="MJXc-Node-332" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.441em; padding-bottom: 0.566em;">(</span></span><span id="MJXc-Node-333" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">n</span></span><span id="MJXc-Node-334" class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.253em; padding-bottom: 0.378em;">≫</span></span><span id="MJXc-Node-335" class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">m</span></span><span id="MJXc-Node-336" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.441em; padding-bottom: 0.566em;">)</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi><mo stretchy="false">(</mo><mi>n</mi><mo>≫</mo><mi>m</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-48">n (n \gg m)</script> raw samples by DC. We also empirically validate the visual privacy and membership privacy of DC-synthesized data by launching both the loss-based and the state-of-the-art likelihood-based membership inference attacks. We envision this work as a milestone for data-efficient and privacy-preserving machine learning.
</div>
</div>

</div>
</td>

</tr>

<tr>
<td>

<div>Outstanding Paper Runner Up</div>

</td>
<td>

<div class="displaycards touchup-date" id="event-16885">




<div style="width:80%;margin:auto;">
<a class="small-title" href="/virtual/2022/poster/16885">Learning inverse folding from millions of predicted structures</a>
</div>
<div class="type_display_name_minus_type"></div>
<div class="author-str">Chloe Hsu · Robert Verkuil · Jason Liu · Zeming Lin · Brian Hie · Tom Sercu · Adam Lerer · Alexander Rives</div>
<div class="author-str higher"></div>
<div class="text-muted touchup-date-div" id="touchup-date-event-16885">Wed 20 Jul 10:30 PM UTC</div>


<p style="font-size:.9em;">[ Hall E ]</p>






<div class="abstract-section">

<div>
<a id="abstract-link-16885" class="abstract-link" data-toggle="collapse" href="#collapse-event-abstract-16885" role="button" aria-expanded="false" aria-controls="collapse-event-abstract-16885">
Abstract <i id="caret-16885" class="fas fa-caret-right" aria-hidden="true"></i>
</a>
</div>


</div>
<div class="collapse" id="collapse-event-abstract-16885">
<div class="abstract-display">
<p>We consider the problem of predicting a protein sequence from its backbone atom coordinates. Machine learning approaches to this problem to date have been limited by the number of available experimentally determined protein structures. We augment training data by nearly three orders of magnitude by predicting structures for 12M protein sequences using AlphaFold2. Trained with this additional data, a sequence-to-sequence transformer with invariant geometric input processing layers achieves 51% native sequence recovery on structurally held-out backbones with 72% recovery for buried residues, an overall improvement of almost 10 percentage points over existing methods. The model generalizes to a variety of more complex tasks including design of protein complexes, partially masked structures, binding interfaces, and multiple states.</p>

</div>
</div>

</div>
</td>

</tr>

<tr>
<td>

<div>Outstanding Paper Runner Up</div>

</td>
<td>

<div class="displaycards touchup-date" id="event-17900">




<div style="width:80%;margin:auto;">
<a class="small-title" href="/virtual/2022/oral/17900">Monarch: Expressive Structured Matrices for Efficient and Accurate Training</a>
</div>
<div class="type_display_name_minus_type"></div>
<div class="author-str">Tri Dao · Beidi Chen · Nimit Sohoni · Arjun Desai · Michael Poli · Jessica Grogan · Alexander Liu · Aniruddh Rao · Atri Rudra · Christopher Re</div>
<div class="author-str higher"></div>
<div class="text-muted touchup-date-div" id="touchup-date-event-17900">Tue 19 Jul 06:00 PM UTC</div>


<p style="font-size:.9em;">[ Ballroom 1 &amp; 2 ]</p>






<div class="abstract-section">

<div>
<a id="abstract-link-17900" class="abstract-link" data-toggle="collapse" href="#collapse-event-abstract-17900" role="button" aria-expanded="false" aria-controls="collapse-event-abstract-17900">
Abstract <i id="caret-17900" class="fas fa-caret-right" aria-hidden="true"></i>
</a>
</div>


</div>
<div class="collapse" id="collapse-event-abstract-17900">
<div class="abstract-display">
<p>Large neural networks excel in many domains, but they are expensive to train and fine-tune. A popular approach to reduce their compute or memory requirements is to replace dense weight matrices with structured ones (e.g., sparse, low-rank, Fourier transform). These methods have not seen widespread adoption (1) in end-to-end training due to unfavorable efficiency--quality tradeoffs, and (2) in dense-to-sparse fine-tuning due to lack of tractable algorithms to approximate a given dense weight matrix. To address these issues, we propose a class of matrices (Monarch) that is hardware-efficient (they are parameterized as products of two block-diagonal matrices for better hardware utilization) and expressive (they can represent many commonly used transforms). Surprisingly, the problem of approximating a dense weight matrix with a Monarch matrix, though nonconvex, has an analytical optimal solution. These properties of Monarch matrices unlock new ways to train and fine-tune sparse and dense models. We empirically validate that Monarch can achieve favorable accuracy-efficiency tradeoffs in several end-to-end sparse training applications: speeding up ViT and GPT-2 training on ImageNet classification and Wikitext-103 language modeling by 2x with comparable model quality, and reducing the error on PDE solving and MRI reconstruction tasks by 40%. In sparse-to-dense training, with a simple technique …</p>
</div>
</div>

</div>
</td>

</tr>

<tr>
<td>

<div>Outstanding Paper Runner Up</div>

</td>
<td>

<div class="displaycards touchup-date" id="event-17380">




<div style="width:80%;margin:auto;">
<a class="small-title" href="/virtual/2022/oral/17380">Minimum Cost Intervention Design for Causal Effect Identification</a>
</div>
<div class="type_display_name_minus_type"></div>
<div class="author-str">Sina Akbari · Jalal Etesami · Negar Kiyavash</div>
<div class="author-str higher"></div>
<div class="text-muted touchup-date-div" id="touchup-date-event-17380">Tue 19 Jul 08:50 PM UTC</div>


<p style="font-size:.9em;">[ Ballroom 3 &amp; 4 ]</p>






<div class="abstract-section">

<div>
<a id="abstract-link-17380" class="abstract-link" data-toggle="collapse" href="#collapse-event-abstract-17380" role="button" aria-expanded="false" aria-controls="collapse-event-abstract-17380">
Abstract <i id="caret-17380" class="fas fa-caret-right" aria-hidden="true"></i>
</a>
</div>


</div>
<div class="collapse" id="collapse-event-abstract-17380">
<div class="abstract-display">
<p>Pearl’s do calculus is a complete axiomatic approach to learn the identifiable causal effects from observational data. When such an effect is not identifiable, it is necessary to perform a collection of often costly interventions in the system to learn the causal effect. In this work, we consider the problem of designing the collection of interventions with the minimum cost to identify the desired effect. First, we prove that this prob-em is NP-complete, and subsequently propose an algorithm that can either find the optimal solution or a logarithmic-factor approximation of it. This is done by establishing a connection between our problem and the minimum hitting set problem. Additionally, we propose several polynomial  time  heuristic  algorithms  to  tackle the computational complexity of the problem. Although these algorithms could potentially stumble on sub-optimal solutions, our simulations show that they achieve small regrets on random graphs.</p>

</div>
</div>

</div>
</td>

</tr>

<tr>
<td>

<div>Outstanding Paper Runner Up</div>

</td>
<td>

<div class="displaycards touchup-date" id="event-17899">




<div style="width:80%;margin:auto;">
<a class="small-title" href="/virtual/2022/poster/17899">Monarch: Expressive Structured Matrices for Efficient and Accurate Training</a>
</div>
<div class="type_display_name_minus_type"></div>
<div class="author-str">Tri Dao · Beidi Chen · Nimit Sohoni · Arjun Desai · Michael Poli · Jessica Grogan · Alexander Liu · Aniruddh Rao · Atri Rudra · Christopher Re</div>
<div class="author-str higher"></div>
<div class="text-muted touchup-date-div" id="touchup-date-event-17899">Tue 19 Jul 10:30 PM UTC</div>


<p style="font-size:.9em;">[ Hall E ]</p>






<div class="abstract-section">

<div>
<a id="abstract-link-17899" class="abstract-link" data-toggle="collapse" href="#collapse-event-abstract-17899" role="button" aria-expanded="false" aria-controls="collapse-event-abstract-17899">
Abstract <i id="caret-17899" class="fas fa-caret-right" aria-hidden="true"></i>
</a>
</div>


</div>
<div class="collapse" id="collapse-event-abstract-17899">
<div class="abstract-display">
<p>Large neural networks excel in many domains, but they are expensive to train and fine-tune. A popular approach to reduce their compute or memory requirements is to replace dense weight matrices with structured ones (e.g., sparse, low-rank, Fourier transform). These methods have not seen widespread adoption (1) in end-to-end training due to unfavorable efficiency--quality tradeoffs, and (2) in dense-to-sparse fine-tuning due to lack of tractable algorithms to approximate a given dense weight matrix. To address these issues, we propose a class of matrices (Monarch) that is hardware-efficient (they are parameterized as products of two block-diagonal matrices for better hardware utilization) and expressive (they can represent many commonly used transforms). Surprisingly, the problem of approximating a dense weight matrix with a Monarch matrix, though nonconvex, has an analytical optimal solution. These properties of Monarch matrices unlock new ways to train and fine-tune sparse and dense models. We empirically validate that Monarch can achieve favorable accuracy-efficiency tradeoffs in several end-to-end sparse training applications: speeding up ViT and GPT-2 training on ImageNet classification and Wikitext-103 language modeling by 2x with comparable model quality, and reducing the error on PDE solving and MRI reconstruction tasks by 40%. In sparse-to-dense training, with a simple technique …</p>
</div>
</div>

</div>
</td>

</tr>

<tr>
<td>

<div>Outstanding Paper Runner Up</div>

</td>
<td>

<div class="displaycards touchup-date" id="event-17174">




<div style="width:80%;margin:auto;">
<a class="small-title" href="/virtual/2022/oral/17174">Adversarially Trained Actor Critic for Offline Reinforcement Learning</a>
</div>
<div class="type_display_name_minus_type"></div>
<div class="author-str">Ching-An Cheng · Tengyang Xie · Nan Jiang · Alekh Agarwal</div>
<div class="author-str higher"></div>
<div class="text-muted touchup-date-div" id="touchup-date-event-17174">Thu 21 Jul 06:15 PM UTC</div>


<p style="font-size:.9em;">[ Room 301 - 303 ]</p>






<div class="abstract-section">

<div>
<a id="abstract-link-17174" class="abstract-link" data-toggle="collapse" href="#collapse-event-abstract-17174" role="button" aria-expanded="false" aria-controls="collapse-event-abstract-17174">
Abstract <i id="caret-17174" class="fas fa-caret-right" aria-hidden="true"></i>
</a>
</div>


</div>
<div class="collapse" id="collapse-event-abstract-17174">
<div class="abstract-display">
<p>We propose Adversarially Trained Actor Critic (ATAC), a new model-free algorithm for offline reinforcement learning (RL) under insufficient data coverage, based on the concept of relative pessimism. ATAC is designed as a two-player Stackelberg game framing of offline RL: A policy actor competes against an adversarially trained value critic, who finds data-consistent scenarios where the actor is inferior to the data-collection behavior policy. We prove that, when the actor attains no regret in the two-player game, running ATAC produces a policy that provably 1) outperforms the behavior policy over a wide range of hyperparameters that control the degree of pessimism, and 2) competes with the best policy covered by data with appropriately chosen hyperparameters. Compared with existing works, notably our framework offers both theoretical guarantees for general function approximation and a deep RL implementation scalable to complex environments and large datasets. In the D4RL benchmark, ATAC consistently outperforms state-of-the-art offline RL algorithms on a range of continuous control tasks.</p>

</div>
</div>

</div>
</td>

</tr>

<tr>
<td>

<div>Outstanding Paper Runner Up</div>

</td>
<td>

<div class="displaycards touchup-date" id="event-16886">




<div style="width:80%;margin:auto;">
<a class="small-title" href="/virtual/2022/oral/16886">Learning inverse folding from millions of predicted structures</a>
</div>
<div class="type_display_name_minus_type"></div>
<div class="author-str">Chloe Hsu · Robert Verkuil · Jason Liu · Zeming Lin · Brian Hie · Tom Sercu · Adam Lerer · Alexander Rives</div>
<div class="author-str higher"></div>
<div class="text-muted touchup-date-div" id="touchup-date-event-16886">Wed 20 Jul 03:05 PM UTC</div>


<p style="font-size:.9em;">[ Hall G ]</p>






<div class="abstract-section">

<div>
<a id="abstract-link-16886" class="abstract-link" data-toggle="collapse" href="#collapse-event-abstract-16886" role="button" aria-expanded="false" aria-controls="collapse-event-abstract-16886">
Abstract <i id="caret-16886" class="fas fa-caret-right" aria-hidden="true"></i>
</a>
</div>


</div>
<div class="collapse" id="collapse-event-abstract-16886">
<div class="abstract-display">
<p>We consider the problem of predicting a protein sequence from its backbone atom coordinates. Machine learning approaches to this problem to date have been limited by the number of available experimentally determined protein structures. We augment training data by nearly three orders of magnitude by predicting structures for 12M protein sequences using AlphaFold2. Trained with this additional data, a sequence-to-sequence transformer with invariant geometric input processing layers achieves 51% native sequence recovery on structurally held-out backbones with 72% recovery for buried residues, an overall improvement of almost 10 percentage points over existing methods. The model generalizes to a variety of more complex tasks including design of protein complexes, partially masked structures, binding interfaces, and multiple states.</p>

</div>
</div>

</div>
</td>

</tr>

<tr>
<td>

<div>Outstanding Paper Runner Up</div>

</td>
<td>

<div class="displaycards touchup-date" id="event-17173">




<div style="width:80%;margin:auto;">
<a class="small-title" href="/virtual/2022/poster/17173">Adversarially Trained Actor Critic for Offline Reinforcement Learning</a>
</div>
<div class="type_display_name_minus_type"></div>
<div class="author-str">Ching-An Cheng · Tengyang Xie · Nan Jiang · Alekh Agarwal</div>
<div class="author-str higher"></div>
<div class="text-muted touchup-date-div" id="touchup-date-event-17173">Thu 21 Jul 10:00 PM UTC</div>


<p style="font-size:.9em;">[ Hall E ]</p>






<div class="abstract-section">

<div>
<a id="abstract-link-17173" class="abstract-link" data-toggle="collapse" href="#collapse-event-abstract-17173" role="button" aria-expanded="false" aria-controls="collapse-event-abstract-17173">
Abstract <i id="caret-17173" class="fas fa-caret-right" aria-hidden="true"></i>
</a>
</div>


</div>
<div class="collapse" id="collapse-event-abstract-17173">
<div class="abstract-display">
<p>We propose Adversarially Trained Actor Critic (ATAC), a new model-free algorithm for offline reinforcement learning (RL) under insufficient data coverage, based on the concept of relative pessimism. ATAC is designed as a two-player Stackelberg game framing of offline RL: A policy actor competes against an adversarially trained value critic, who finds data-consistent scenarios where the actor is inferior to the data-collection behavior policy. We prove that, when the actor attains no regret in the two-player game, running ATAC produces a policy that provably 1) outperforms the behavior policy over a wide range of hyperparameters that control the degree of pessimism, and 2) competes with the best policy covered by data with appropriately chosen hyperparameters. Compared with existing works, notably our framework offers both theoretical guarantees for general function approximation and a deep RL implementation scalable to complex environments and large datasets. In the D4RL benchmark, ATAC consistently outperforms state-of-the-art offline RL algorithms on a range of continuous control tasks.</p>

</div>
</div>

</div>
</td>

</tr>

<tr>
<td>

<div>Outstanding Paper Runner Up</div>

</td>
<td>

<div class="displaycards touchup-date" id="event-17379">




<div style="width:80%;margin:auto;">
<a class="small-title" href="/virtual/2022/poster/17379">Minimum Cost Intervention Design for Causal Effect Identification</a>
</div>
<div class="type_display_name_minus_type"></div>
<div class="author-str">Sina Akbari · Jalal Etesami · Negar Kiyavash</div>
<div class="author-str higher"></div>
<div class="text-muted touchup-date-div" id="touchup-date-event-17379">Tue 19 Jul 10:30 PM UTC</div>


<p style="font-size:.9em;">[ Hall E ]</p>






<div class="abstract-section">

<div>
<a id="abstract-link-17379" class="abstract-link" data-toggle="collapse" href="#collapse-event-abstract-17379" role="button" aria-expanded="false" aria-controls="collapse-event-abstract-17379">
Abstract <i id="caret-17379" class="fas fa-caret-right" aria-hidden="true"></i>
</a>
</div>


</div>
<div class="collapse" id="collapse-event-abstract-17379">
<div class="abstract-display">
<p>Pearl’s do calculus is a complete axiomatic approach to learn the identifiable causal effects from observational data. When such an effect is not identifiable, it is necessary to perform a collection of often costly interventions in the system to learn the causal effect. In this work, we consider the problem of designing the collection of interventions with the minimum cost to identify the desired effect. First, we prove that this prob-em is NP-complete, and subsequently propose an algorithm that can either find the optimal solution or a logarithmic-factor approximation of it. This is done by establishing a connection between our problem and the minimum hitting set problem. Additionally, we propose several polynomial  time  heuristic  algorithms  to  tackle the computational complexity of the problem. Although these algorithms could potentially stumble on sub-optimal solutions, our simulations show that they achieve small regrets on random graphs.</p>

</div>
</div>

</div>
</td>

</tr>

<tr>
<td>

<div>Outstanding Paper Runner Up</div>

</td>
<td>

<div class="displaycards touchup-date" id="event-17203">




<div style="width:80%;margin:auto;">
<a class="small-title" href="/virtual/2022/poster/17203">Active fairness auditing</a>
</div>
<div class="type_display_name_minus_type"></div>
<div class="author-str">Tom Yan · Chicheng Zhang</div>
<div class="author-str higher"></div>
<div class="text-muted touchup-date-div" id="touchup-date-event-17203">Wed 20 Jul 10:30 PM UTC</div>


<p style="font-size:.9em;">[ Hall E ]</p>






<div class="abstract-section">

<div>
<a id="abstract-link-17203" class="abstract-link" data-toggle="collapse" href="#collapse-event-abstract-17203" role="button" aria-expanded="false" aria-controls="collapse-event-abstract-17203">
Abstract <i id="caret-17203" class="fas fa-caret-right" aria-hidden="true"></i>
</a>
</div>


</div>
<div class="collapse" id="collapse-event-abstract-17203">
<div class="abstract-display">
<p>The fast spreading adoption of machine learning (ML) by companies across industries poses significant regulatory challenges. One such challenge is scalability: how can regulatory bodies efficiently \emph{audit} these ML models, ensuring that they are fair? In this paper, we initiate the study of query-based auditing algorithms that can estimate the demographic parity of ML models in a query-efficient manner. We propose an optimal deterministic algorithm, as well as a practical randomized, oracle-efficient algorithm with comparable guarantees. Furthermore, we make inroads into understanding the optimal query complexity of randomized active fairness estimation algorithms. Our first exploration of active fairness estimation aims to put AI governance on firmer theoretical foundations.</p>

</div>
</div>

</div>
</td>

</tr>

<tr>
<td>

<div>Outstanding Paper Runner Up</div>

</td>
<td>

<div class="displaycards touchup-date" id="event-17204">




<div style="width:80%;margin:auto;">
<a class="small-title" href="/virtual/2022/oral/17204">Active fairness auditing</a>
</div>
<div class="type_display_name_minus_type"></div>
<div class="author-str">Tom Yan · Chicheng Zhang</div>
<div class="author-str higher"></div>
<div class="text-muted touchup-date-div" id="touchup-date-event-17204">Wed 20 Jul 05:50 PM UTC</div>


<p style="font-size:.9em;">[ Ballroom 3 &amp; 4 ]</p>






<div class="abstract-section">

<div>
<a id="abstract-link-17204" class="abstract-link" data-toggle="collapse" href="#collapse-event-abstract-17204" role="button" aria-expanded="false" aria-controls="collapse-event-abstract-17204">
Abstract <i id="caret-17204" class="fas fa-caret-right" aria-hidden="true"></i>
</a>
</div>


</div>
<div class="collapse" id="collapse-event-abstract-17204">
<div class="abstract-display">
<p>The fast spreading adoption of machine learning (ML) by companies across industries poses significant regulatory challenges. One such challenge is scalability: how can regulatory bodies efficiently \emph{audit} these ML models, ensuring that they are fair? In this paper, we initiate the study of query-based auditing algorithms that can estimate the demographic parity of ML models in a query-efficient manner. We propose an optimal deterministic algorithm, as well as a practical randomized, oracle-efficient algorithm with comparable guarantees. Furthermore, we make inroads into understanding the optimal query complexity of randomized active fairness estimation algorithms. Our first exploration of active fairness estimation aims to put AI governance on firmer theoretical foundations.</p>

</div>
</div>

</div>
</td>

</tr>

</tbody></table>


<form method="POST"><input type="hidden" name="csrfmiddlewaretoken" value="Fxk43P1mlPpqMUG539oaDW2VzvBs8n9teKbelXtDjn72vM50btCXHYhaHY4KDYgz"></form>
</div>