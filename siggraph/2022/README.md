<!-- src: https://blog.siggraph.org/2022/07/siggraph-2022-technical-papers-awards-best-papers-and-honorable-mentions.html/ -->
<div class="et_pb_row et_pb_row_0_tb_body dbdb_default_mobile_width">
<div class="et_pb_column et_pb_column_4_4 et_pb_column_0_tb_body  et_pb_css_mix_blend_mode_passthrough et-last-child">




<div class="et_pb_module et_pb_post_title et_pb_post_title_0_tb_body et_pb_bg_layout_light  et_pb_text_align_left">





<div class="et_pb_title_container">
<h1 class="entry-title">SIGGRAPH 2022 Technical Papers Awards: Best Papers and Honorable Mentions</h1><p class="et_pb_title_meta_container"> by <span class="author vcard"><a href="https://blog.siggraph.org/author/mmanghera/" title="Posts by SIGGRAPH Conferences" rel="author">SIGGRAPH Conferences</a></span> | <span class="published">6 July 2022</span> | <a href="https://blog.siggraph.org/category/conferences/" rel="category tag">Conferences</a>, <a href="https://blog.siggraph.org/category/research/" rel="category tag">Research</a></p>
</div>

</div><div class="et_pb_module et_pb_image et_pb_image_0_tb_body">




<span class="et_pb_image_wrap "><img fetchpriority="high" decoding="async" width="850" height="480" src="https://blog.siggraph.org/wp-content/uploads/2022/07/Blog_2022-Technical-Papers-Awards.png" alt="" title="Blog_2022-Technical Papers Awards" srcset="https://blog.siggraph.org/wp-content/uploads/2022/07/Blog_2022-Technical-Papers-Awards.png 850w, https://blog.siggraph.org/wp-content/uploads/2022/07/Blog_2022-Technical-Papers-Awards-300x169.png 300w, https://blog.siggraph.org/wp-content/uploads/2022/07/Blog_2022-Technical-Papers-Awards-768x434.png 768w, https://blog.siggraph.org/wp-content/uploads/2022/07/Blog_2022-Technical-Papers-Awards-640x361.png 640w" sizes="(max-width: 850px) 100vw, 850px" class="wp-image-14818"></span>
<div id="pac_dih__image_details_0" class="pac_dih__image_details "></div></div>
<div class="et_pb_module et_pb_post_content et_pb_post_content_0_tb_body">





<p class="has-text-align-right"><em>“Image Features Influence Reaction Time: A Learned Probabilistic Perceptual Model for Saccade Latency” ©&nbsp;Budmonde Duinkharjav, New York University; Praneeth Chakravarthula, Princeton University, University of North Carolina at Chapel Hill; Rachel Brown, Anjul Patney, NVIDIA Research; Qi Sun, New York University</em></p>



<p>Technical Papers is a pillar of the annual SIGGRAPH conference where scientists and researchers come to disseminate new scholarly work and propel the industry forward. This year, the program offered two ways to submit research — Journal Papers, which is the continuation of the same program from previous years, and Conference Papers, where ideas are shared in a shorter format but still make a significant impact.</p>



<p>The new format is not the only addition to SIGGRAPH 2022. New this year, SIGGRAPH 2022 introduces the Technical Papers awards for Best Papers and Honorable Mentions. These papers were selected for their research prominence and innovative contributions to the future of research in computer graphics and interactive techniques. The Best Papers Award winners and the Honorable Mentions will be honored during the SIGGRAPH 2022 closing session in Vancouver on Thursday, 11 August.</p>



<p>SIGGRAPH 2022 Technical Papers Chair Niloy Mitra is thrilled to showcase these award-winning papers and thanks the selection committee, SIGGRAPH 2023 Technical Papers Chair Alla Sheffer and a selection of senior International Papers Committee members, who chose the Best Papers and Honorable Mentions out of a pool of hundreds.</p>



<p>Celebrate the winners and honorable mentions below to get a taste of what to expect during SIGGRAPH 2022.</p>



<h2 class="wp-block-heading"><strong>Best Papers</strong></h2>



<p><strong>Image Features Influence Reaction Time: A Learned Probabilistic Perceptual Model for Saccade Latency</strong></p>



<p>We present a neurologically inspired perceptual model to predict eye reaction latency as a function of observed image characteristics on screen. Our model may serve as a metric for predicting and altering reaction latencies in esports and AR/VR applications.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p>



<p><em>Budmonde Duinkharjav, New York University; Praneeth Chakravarthula, Princeton University, University of North Carolina at Chapel Hill; Rachel Brown, Anjul Patney, NVIDIA Research; Qi Sun, New York University</em></p>



<p><strong>CLIPasso: Semantically Aware Object Sketching</strong></p>



<p>We present CLIPasso, a method for sketching objects at different levels of abstraction. We define a sketch as a set of strokes and use a differentiable rasterizer to optimize the strokes’ parameters with respect to a CLIP-based perceptual loss. The abstraction degree is controlled by varying the number of strokes.</p>



<p><em>Yael Vinker, Tel Aviv University, École Polytechnique Fédérale de Lausanne; Ehsan Pajouheshgar, École Polytechnique Fédérale de Lausanne; Jessica Y. Bo, École Polytechnique Fédérale de Lausanne, ETH; Roman Christian Bachmann, École Polytechnique Fédérale de Lausanne; Amit Bermano, Tel Aviv University; Daniel Cohen-Or, Tel Aviv University; Amir Zamir, EPFL; Ariel Shamir, Reichman University</em><em></em></p>



<p><strong>Instant Neural Graphics Primitives with a Multiresolution Hash Encoding&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</strong></p>



<p>Neural networks emerged as high-quality representations of graphics primitives, such as signed distance functions, light fields, textures, and the likes. Our approach can train such primitives in seconds and render them in milliseconds, allowing their use in the inner loops of graphics algorithms where they previously may have been discounted.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p>



<p><em>Thomas Müller, Alex Evans, Christoph Schied, Alexander Keller, NVIDIA</em></p>



<p><strong>Spelunking the Deep: Guaranteed Queries on General Neural Implicit Surfaces&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</strong></p>



<p>This work develops geometric queries such as ray casting, closest-point, intersection testing, building spatial hierarchies, etc., for neural implicit surfaces. The key tool is range analysis, which automatically computes local bounds on the neural network output. The resulting queries have guaranteed accuracy, even on randomly initialized networks.</p>



<p><em>Nicholas Sharp, University of Toronto; Alec Jacobson, University of Toronto, Adobe Research</em></p>



<p><strong>DeepPhase: Periodic Autoencoders for Learning Motion Phase Manifolds&nbsp;&nbsp;&nbsp;&nbsp;</strong></p>



<p>We present the Periodic Autoencoder that can learn the spatial-temporal structure of body movements from unstructured motion data. The network produces a multi-dimensional phase manifold that helps enhance neural character controllers and motion matching for a variety of tasks, including diverse locomotion, style-based movements, dancing to music, or football dribbling.</p>



<p><em>Sebastian Starke, University of Edinburgh, Electronic Arts; Ian Mason, University of Edinburgh; Taku Komura, University of Hong Kong</em></p>



<h2 class="wp-block-heading"><strong>Honorable Mentions</strong></h2>



<p><strong>Joint Neural Phase Retrieval and Compression for Energy- and Computation-efficient Holography on the Edge&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</strong></p>



<p>We propose a framework that jointly generates and compresses phase-only holograms and achieves high remote transmission efficiency and image reconstruction quality on edge devices. By asymmetrical distribution of computation between information encoding process and phase decoding procedure, we demonstrate low computation and energy costs on edge holographic display devices.</p>



<p><em>Yujie Wang, Shandong University, Peking University; Praneeth Chakravarthula, Princeton University, University of North Carolina at Chapel Hill; Qi Sun, New York University; Baoquan Chen, Peking University</em></p>



<p><strong>Computational Design of High-level Interlocking Puzzles&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</strong></p>



<p>We present a computational approach for designing high-level interlocking puzzles according to the user-specified puzzle shape, number of puzzle pieces, and level of difficulty. Our key idea is to leverage a new disassembly graph to represent all possible configurations of an interlocking puzzle and to guide the puzzle design process.</p>



<p><em>Rulin Chen, Singapore University of Technology and Design; Ziqi Wang, ETH Zürich; Peng Song, Singapore University of Technology and Design; Bernd Bickel, IST Austria</em></p>



<p><strong>Sparse Ellipsometry: Portable Acquisition of Polarimetric SVBRDF and Shape With Unstructured Flash Photography&nbsp;&nbsp;</strong></p>



<p>We present sparse ellipsometry, a portable polarimetric acquisition method that captures both polarimetric SVBRDF and 3D shape simultaneously. We also develop a complete polarimetric SVBRDF model that includes diffuse and specular components, as well as single scattering, and devise a novel polarimetric inverse rendering algorithm.</p>



<p><em>Inseung Hwang, Daniel S. Jeon, Korea Advanced Institute of Science and Technology (KAIST); Adolfo Muñoz, Diego Gutierrez, Universidad de Zaragoza – I3A; Xin Tong, Microsoft Research Asia; Min H. Kim, Korea Advanced Institute of Science and Technology (KAIST)</em></p>



<p><strong>Umbrella Meshes: Volumetric Elastic Mechanisms for Freeform Shape Deployment</strong></p>



<p>Umbrella meshes are bending-active structures that can be optimized to deploy from a compact fabrication state into a freeform design surface. We propose a complete inverse-design pipeline incorporating physics-based simulation to accurately model the elastic deformation of the structure when solving for the geometric parameters of the constituent unit-cell mechanisms.</p>



<p><em>Yingying Ren, Uday Kusupati, EPFL; Julian Panetta, UC Davis; Florin Isvoranu, Davide Pellis, EPFL; Tian Chen, University of Houston; Mark Pauly, EPFL</em></p>



<p><strong>Sketch2Pose: Estimating a 3D Character Pose From a Bitmap Sketch&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</strong></p>



<p>Artists often capture character poses via raster sketches then use these drawings as a reference while painstakingly posing a 3D character in a 3D animation software. We propose the first system for algorithmically inferring a 3D character pose from a single bitmap sketch, producing poses consistent with viewer expectations.</p>



<p><em>Kirill Brodt, Mikhail Bessmeltsev, University of Montreal</em></p>



<p><strong>Facial Hair Tracking for High Fidelity Performance Capture&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</strong></p>



<p>Facial hair is a largely overlooked problem in facial performance capture, requiring actors to shave clean before a capture session. We propose the first method that can reconstruct and track 3D facial hair fibers and approximate the underlying skin during dynamic facial performances.</p>



<p><em>Sebastian Winberg, ETH Zürich, DisneyResearch|Studios; Gaspard Zoss, Prashanth Chandran, Paulo Gotardo, Derek Bradley, DisneyResearch|Studios</em></p>



<p><strong>Towards Practical Physical-optics Rendering&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</strong></p>



<p>We present a practical framework for physical light transport, capable of reproducing global diffraction and wave-interference effects in rendering. Unlike existing methods, we are able to render realistic, elaborate scenes with complex coherence-aware materials, account for the light’s wave properties throughout, and at a performance approaching radiometric renderers.</p>



<p><em>Shlomi Steinberg, Pradeep Sen, Ling-Qi Yan, University of California Santa Barbara</em></p>



<p><strong>Free2CAD: Parsing Freehand Drawings Into CAD Commands&nbsp;</strong></p>



<p>We present Free2CAD, wherein the user can sketch a shape and our system parses the input strokes into CAD commands that reproduce the sketched object. Technically, we cast sketch-based CAD modeling as a sequence-to-sequence translation problem where input pen strokes are grouped to correspond to individual CAD operations.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p>



<p><em>Changjian Li, INRIA, University Côte d’Azur, University College London (UCL); Hao Pan, Microsoft Research Asia; Adrien Bousseau, INRIA, University Côte d’Azur; Niloy Mitra, University College London (UCL), Adobe Research</em></p>



<p><strong>Grid-free Monte Carlo for PDEs With Spatially Varying Coefficients&nbsp;&nbsp;&nbsp;&nbsp;</strong></p>



<p>We describe a method to solve partial differential equations (PDEs) with spatially varying coefficients on complex geometric domains without any approximation of the geometry or coefficient functions. Our main contribution is to extend the walk on spheres (WoS) algorithm by drawing inspiration from Monte Carlo techniques for volumetric rendering.&nbsp;</p>



<p><em>Rohan Sawhney, Carnegie Mellon University; Dario Seyb, Wojciech Jarosz, Dartmouth College; Keenan Crane, Carnegie Mellon University</em></p>



<p><strong>Implicit Neural Representation for Physics-driven Actuated Soft Bodies&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</strong></p>



<p>We present an implicit formulation to control active soft bodies by defining a function that enables a continuous mapping from a spatial point in the material space to the control parameters. This allows us to capture the dominant frequencies of the signal, making the method discretization agnostic and widely applicable.</p>



<p><em>Lingchen Yang, Byungsoo Kim, ETH Zurich; Gaspard Zoss, DisneyResearch|Studios; Baran Gözcü, Markus Gross, Barbara Solenthaler, ETH Zurich</em></p>



<p><em>Ready to experience the latest CG and interactive techniques research in person and online? </em><a href="https://s2022.siggraph.org/register/"><em>Register for SIGGRAPH 2022</em></a><em> and gain access to 133 Journal Papers, 61 Conference Papers, and 53 ACM Transactions on Graphics papers to be presented across 31 sessions. Access the virtual pre-recorded presentations and participate in the virtual Technical Papers Fast Forward. Join us in Vancouver to participate in roundtable discussions with paper authors. </em><a href="https://s2022.siggraph.org/full-program/?filter1=sstype128"><em>View all Technical Papers content on the SIGGRAPH 2022 website</em></a><em> to start planning your conference experience.</em></p>

</div><div class="et_pb_module et_pb_post_title et_pb_post_title_1_tb_body post-meta-tags et_pb_bg_layout_light  et_pb_text_align_left">





<div class="et_pb_title_container">

<p class="et_pb_title_meta_container"><span class="dbdb_posttitle_tags_separator">   </span><span class="dbdb_posttitle_tags"><a href="https://blog.siggraph.org/tag/alla-sheffer/" rel="tag">Alla Sheffer</a> <a href="https://blog.siggraph.org/tag/awards/" rel="tag">Awards</a> <a href="https://blog.siggraph.org/tag/best-papers/" rel="tag">Best Papers</a> <a href="https://blog.siggraph.org/tag/niloy-mitra/" rel="tag">Niloy Mitra</a> <a href="https://blog.siggraph.org/tag/research/" rel="tag">Research</a> <a href="https://blog.siggraph.org/tag/siggraph-2022/" rel="tag">SIGGRAPH 2022</a> <a href="https://blog.siggraph.org/tag/technical-papers/" rel="tag">Technical Papers</a> <a href="https://blog.siggraph.org/tag/technical-papers-awards/" rel="tag">Technical Papers awards</a></span></p></div>

</div>
</div>




</div>