<!-- src: https://blog.siggraph.org/2023/07/siggraph-2023-technical-papers-awards-best-papers-honorable-mentions-and-test-of-time.html/ -->

<div class="et_pb_row et_pb_row_0_tb_body dbdb_default_mobile_width">
<div class="et_pb_column et_pb_column_4_4 et_pb_column_0_tb_body  et_pb_css_mix_blend_mode_passthrough et-last-child">




<div class="et_pb_module et_pb_post_title et_pb_post_title_0_tb_body et_pb_bg_layout_light  et_pb_text_align_left">





<div class="et_pb_title_container">
<h1 class="entry-title">SIGGRAPH 2023 Technical Papers Awards: Best Papers, Honorable Mentions, and Test-of-Time</h1><p class="et_pb_title_meta_container"> by <span class="author vcard"><a href="https://blog.siggraph.org/author/mmanghera/" title="Posts by SIGGRAPH Conferences" rel="author">SIGGRAPH Conferences</a></span> | <span class="published">6 July 2023</span> | <a href="https://blog.siggraph.org/category/conferences/" rel="category tag">Conferences</a>, <a href="https://blog.siggraph.org/category/research/" rel="category tag">Research</a></p>
</div>

</div><div class="et_pb_module et_pb_image et_pb_image_0_tb_body">




<span class="et_pb_image_wrap "><img fetchpriority="high" decoding="async" width="850" height="480" src="https://blog.siggraph.org/wp-content/uploads/2023/07/Blog-Header-1.png" alt="" title="Blog Header (1)" srcset="https://blog.siggraph.org/wp-content/uploads/2023/07/Blog-Header-1.png 850w, https://blog.siggraph.org/wp-content/uploads/2023/07/Blog-Header-1-300x169.png 300w, https://blog.siggraph.org/wp-content/uploads/2023/07/Blog-Header-1-768x434.png 768w, https://blog.siggraph.org/wp-content/uploads/2023/07/Blog-Header-1-640x361.png 640w" sizes="(max-width: 850px) 100vw, 850px" class="wp-image-16492"></span>
<div id="pac_dih__image_details_0" class="pac_dih__image_details "></div></div>
<div class="et_pb_module et_pb_post_content et_pb_post_content_0_tb_body">





<p class="has-text-align-right"><em>Image credit: “Split-Lohmann Multifocal Displays”</em> <em><em>© </em>3D scene model courtesy of “Entity Designer” on Blender Market under “Royalty free license” (See: https://blendermarket.com/products/dark-interior-scene)</em></p>



<p>The <a href="https://s2023.siggraph.org/program/technical-papers/" target="_blank" rel="noreferrer noopener">Technical Papers program</a> has been at the core of SIGGRAPH since the very first conference 50 years ago. It is the premier international venue for disseminating and discussing innovative scholarly work in animation, simulation, imaging, geometry, modeling, rendering, human-computer interaction, haptics, fabrication, robotics, visualization, audio, optics, programming languages, immersive experiences, and machine learning for visual computing to name a few.</p>



<p>Following on the success of SIGGRAPH 2022, SIGGRAPH 2023 accepted submissions to two integrated paper tracks: Journal (<em>ACM Transactions on Graphics</em>) and Conference. Additionally, SIGGRAPH 2023 is continuing the new tradition of awards for Best Papers and Honorable Mentions. These papers were selected for their research prominence and new contributions to the future of research in computer graphics and interactive techniques.</p>



<p>SIGGRAPH 2023 Technical Papers Chair Alla Sheffer is thrilled to highlight these award-winning papers and thanks the selection committee who chose the Best Papers and Honorable Mentions out of a pool of hundreds.</p>



<p>Learn more about the Best Papers and Honorable Mentions below, and get ready to explore what’s next in research at SIGGRAPH 2023. Plus, learn more about the new ACM SIGGRAPH award, Test-of-Time, and its recipients. But first, check out the SIGGRAPH 2023 Technical Papers Trailer! </p>



<!-- <figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<div class="fluid-width-video-wrapper" style="padding-top: 56.2963%;"><iframe title="SIGGRAPH 2023 Technical Papers Trailer" src="https://www.youtube.com/embed/VBZ2sDxvZQE?feature=oembed" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="" name="fitvid0"></iframe></div>
</div></figure> -->
[![](https://img.youtube.com/vi/VBZ2sDxvZQE/maxresdefault.jpg)](https://www.youtube.com/embed/VBZ2sDxvZQE?feature=oembed)



<h2 class="wp-block-heading"><strong>Best Papers</strong></h2>



<h4 class="wp-block-heading"><strong>Split-Lohmann Multifocal Displays</strong></h4>



<p>This work describes a near-eye 3D display that instantaneously creates a virtual world, fully supporting the human eye’s native ability to focus on content placed at different distances. This capability enables a viewer to experience 3D videos and interactive games at a previously unattainable level of immersion.</p>



<p><em>Yingsi Qin, Wei-Yu Chen, Matthew O’Toole, Aswin C. Sankaranarayanan, Carnegie Mellon University</em></p>



<h4 class="wp-block-heading"><strong>Differentiable Stripe Patterns for Inverse Design of Structured Surfaces</strong></h4>



<p>We introduce Differentiable Stripe Patterns — a computational approach for automated design of physical surfaces structured with stripe-shaped, bi-material distributions. We propose a gradient-based optimization tool to automatically compute stripe patterns that best approximate macromechanical performance goals.</p>



<p><em>Juan Sebastian Montes Maestre, Yinwei Du, Ronan Hinchet, Stelian Coros, Bernhard Thomaszewski, ETH Zürich</em></p>



<h4 class="wp-block-heading"><strong>Globally Consistent Normal Orientation for Point Clouds by Regularizing the Winding-number Field</strong></h4>



<p>We propose a smooth objective function to characterize the requirements of an acceptable winding-number field, which allows one to find the globally consistent normal orientations starting from a set of completely random normals.</p>



<p><em>Rui Xu, Shandong University; Zhiyang Dou, The University of Hong Kong; Ningna Wang, The University of Texas at Dallas; Shiqing Xin, Shandong University; Shuangmin Chen, </em><em>Qingdao University of Science and Technology; Mingyan Jiang, Shandong University; Xiaohu Guo, The University of Texas at Dallas; Wenping Wang, Texas A&amp;M University; Changhe Tu, Shandong University</em></p>



<h4 class="wp-block-heading"><strong>3D Gaussian Splatting for Real-time Radiance Field Rendering</strong></h4>



<p>Our method allows real-time rendering (&gt;= 30fps) of radiance fields with high visual quality. We represent scenes accurately with 3D Gaussians allowing efficient optimization. Our visibility-aware rendering accelerates training which is as fast as the fastest previous methods for equivalent quality. An additional hour of training provides state-of-the-art quality.</p>



<p><em>Bernhard Kerbl, Inria, Université Côte d’Azur; Georgios Kopanas, Inria, Université Côte d’Azur; Thomas Leimkuehler, </em><em>Max-Planck-Institut für Informatik; George Drettakis, Inria, Université Côte d’Azur</em></p>



<h4 class="wp-block-heading"><strong>DOC: Differentiable Optimal Control for Retargeting Motions Onto Legged Robots</strong></h4>



<p>We present a Differentiable Optimal Control (DOC) framework that facilitates the computation of analytical derivatives of optimal control and state trajectories with respect to user-defined parameters. We demonstrate the utility of DOC by retargeting mocap and animation data onto a family of legged robots of varying proportions and mass distribution.</p>



<p><em>Ruben Grandia, Disney Research Imagineering; Farbod Farshidian, ETH Zürich; Espen Knoop, Disney Research Imagineering; Christian Schumacher, Disney Research Imagineering; Marco Hutter, ETH Zürich; Moritz Bächer, Disney Research Imagineering</em></p>



<h2 class="wp-block-heading"><strong>Honorable Mentions</strong></h2>



<h4 class="wp-block-heading"><strong>GestureDiffuCLIP: Gesture Diffusion Model With CLIP Latents</strong></h4>



<p>We introduce GestureDiffuCLIP, a CLIP-guided, co-speech gesture synthesis system that creates stylized gestures in harmony with speech semantics and rhythm using arbitrary style prompts. Our highly adaptable system supports style prompts in the form of short texts, motion sequences, or video clips and provides body part-specific style control.</p>



<p><em>Tenglong Ao, Zeyi Zhang, Libin Liu, Peking University</em></p>



<h4 class="wp-block-heading"><strong>Word-as-image for Semantic Typography</strong></h4>



<p>Word-as-image is a technique where a word illustration presents a visualization of the meaning of the word, while also preserving its readability. We present a method to create word-as-image illustrations automatically. We optimize the outline of each letter to convey the desired concept, guided by a pretrained Stable Diffusion model.</p>



<p><em>Shir Iluz, Tel Aviv University; Yael Vinker, Tel Aviv University; Amir Hertz, Tel Aviv University; Daniel Berio, Goldsmiths University of London; Daniel Cohen-Or, Tel Aviv University; Ariel Shamir, Reichman University</em></p>



<h4 class="wp-block-heading"><strong>Sag-Free Initialization for Strand-Based Hybrid Hair Simulation</strong></h4>



<p>This paper proposes a novel four-stage, sag-free initialization framework to solve stable quasistatic configurations for hybrid, strand-based hair dynamic systems. Our results show that our method successfully prevents sagging on various hairstyles and has minimal impact on the hair motion during simulation.</p>



<p><em>Jerry Hsu, University of Utah, LightSpeed Studios, Tencent America; Tongtong Wang, LightSpeed Studios, Tencent America; Zherong Pan, LightSpeed Studios, Tencent America; Xifeng Gao, LightSpeed Studios, Tencent America; Cem Yuksel, </em><em>University of Utah, Roblox Research; Kui Wu, LightSpeed Studios, Tencent America&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</em></p>



<h4 class="wp-block-heading"><strong>Deployable Strip Structures</strong></h4>



<p>C-meshes capture kinetic structures deployable from a collapsed state. They enjoy rich geometry and surprising relations to differential geometry, in particular surfaces with the linear Weingarten property. We provide tools for designing and exploring the shape space of C-meshes, and we present architectural paneling applications.</p>



<p><em>Daoming Liu, King Abdullah University of Science and Technology (KAUST); Davide Pellis, ISTI-CNR; Yu-Chou Chiang, National Chung Hsing University; Florian Rist, King Abdullah University of Science and Technology (KAUST); Johannes Wallner, TU Graz; Helmut Pottmann, King Abdullah University of Science and Technology (KAUST)</em></p>



<h4 class="wp-block-heading"><strong>Towards Attention-Aware Rendering</strong></h4>



<p>Existing perceptual models used in foveated graphics neglect the effects of visual attention. We introduce the first attention-aware model of contrast sensitivity and motivate the development of future foveation models, demonstrating that tolerance for foveation is significantly higher when the user is concentrating on a task in the fovea.</p>



<p><em>Brooke Krajancich, </em><em>Stanford University; Petr Kellnhofer, TU Delft; Gordon Wetzstein, Stanford University</em></p>



<h4 class="wp-block-heading"><strong>Random-access Neural Compression of Material Textures</strong></h4>



<p>This work introduces a neural compression technique for mipmapped material texture sets, offering significantly better compression than BCx at comparable quality and even surpassing entropy-coded AVIF and JPEG XL at low bitrates. Our method uses small, optimized neural networks for efficient compression, real-time decompression, and random access on GPUs.</p>



<p><em>Karthik Vaidyanathan, Marco Salvi, Bartlomiej Wronski, Tomas Akenine-Moller, Pontus Ebelin, Aaron Lefohn, NVIDIA</em></p>



<h4 class="wp-block-heading"><strong>Learning Physically Simulated Tennis Skills From Broadcast Videos</strong></h4>



<p>We present a system to learn diverse and complex tennis skills leveraging large-scale but lower-quality motions harvested from broadcast tennis videos for physically simulated characters to hit the ball to target positions with high accuracy and successfully conduct competitive rally play that includes a range of shot types and spins.</p>



<p><em>Haotian Zhang, Stanford University; Ye Yuan, NVIDIA; Viktor Makoviychuk, NVIDIA; Yunrong Guo, NVIDIA; Sanja Fidler, NVIDIA, University of Toronto; Xue Bin Peng, NVIDIA, Simon Fraser University; Kayvon Fatahalian, Stanford University</em></p>



<h4 class="wp-block-heading"><strong>Min-Deviation-Flow in Bi-directed Graphs for T-Mesh Quantization</strong></h4>



<p>Integer optimization problems for T-mesh quantization are central to state-of-the-art quad-meshing methods. We show how their structure allows modeling as generalized network flow problems in multiple ways. Our novel approximate and exact solvers achieve dramatic speed-ups over general solvers and have applications beyond T-mesh quantization.</p>



<p><em>Martin Heistermann, University of Bern; Jethro Warnett, University of Oxford; David Bommes, University of Bern</em></p>



<h2 class="wp-block-heading"><strong>Test-of-Time Awards</strong></h2>



<p>ACM SIGGRAPH is delighted to announce the 2023 Test-of-Time Award papers that have had a significant and lasting impact on computer graphics and interactive techniques over at least a decade. This is the first year of this annual award. For 2023, the papers presented at SIGGRAPH conferences from 2011 to 2013 were considered by the Test-of-Time Award committee and the committee selected four winning papers.</p>



<h4 class="wp-block-heading"><strong>Functional Maps: A Flexible Representation of Maps Between Shapes (2012)</strong></h4>



<p>Establishing correspondences between pairs of shapes is a fundamental step for shape inference and manipulation. This paper introduced a new representation of functional maps and has sparked a large volume of follow-on research on shape matching. <a href="https://doi.org/10.1145/2185520.2185526" target="_blank" rel="noreferrer noopener">Read the paper on the ACM Digital Library</a>.</p>



<p><em>Maks Ovsjanikov, Mirela Ben-Chen, Justin Solomon, Adrian Butscher, Leonidas Guibas</em><em></em></p>



<h4 class="wp-block-heading"><strong>Eulerian Video Magnification for Revealing Subtle Changes in the World (2012)</strong></h4>



<p>This paper shows that cameras can capture subtle, yet important, motion that is too subtle for the human eye to see. Follow-on studies have found many application areas including video surveillance, visual vibrometry, and visual microphones. <a href="https://doi.org/10.1145/2185520.2185561" target="_blank" rel="noreferrer noopener">Read the paper on the ACM Digital Library</a>.</p>



<p><em>Hao-Yu Wu, Michael Rubinstein, Eugene Shih, John Guttag, Frédo Durand, William Freeman</em><em></em></p>



<h4 class="wp-block-heading"><strong>HDR-VDP-2: A Calibrated Visual Metric for Visibility and Quality Predictions in All Luminance Conditions (2011)</strong></h4>



<p>The metric presented in this paper includes a calibrated model of human vision across different luminance conditions and has become the default standard metric to predict the visibility and quality of images for a wide range of intensities.&nbsp;<a href="https://doi.org/10.1145/2010324.1964935" target="_blank" rel="noreferrer noopener">Read the paper on the ACM Digital Library</a>.</p>



<p><em>Rafal Mantiuk, Kil Joong Kim, Allan G. Rempel, Wolfgang Heidrich</em><em></em></p>



<h4 class="wp-block-heading"><strong>Optimizing Locomotion Controllers Using Biologically-based Actuators and Objectives (2012)</strong></h4>



<p>This paper introduced an innovative way to simulate human locomotion at the musculoskeletal level and inspired new research directions in how we view human movements and the extent to which they can be simulated. <a href="https://doi.org/10.1145/2185520.2185521" target="_blank" rel="noreferrer noopener">Read the paper on the ACM Digital Library</a>.</p>



<p><em>Jack M. Wang, Samuel R. Hamner, Scott L. Delp, Vladlen Koltun</em><em></em></p>



<p><em>Thank you to Jehee Lee for gathering information about the Test-of-Time Awards. </em></p>



<hr class="wp-block-separator has-alpha-channel-opacity">



<p><em><a href="https://s2023.siggraph.org/register/" target="_blank" rel="noreferrer noopener">Register for SIGGRAPH 2023</a>, taking place 6–10 August in Los Angeles, to access the best of the best scholarly research in computer graphics and interactive techniques. Don’t miss a moment of Technical Papers excellence. <a href="https://s2023.siggraph.org/full-program/?filter1=sstype101" target="_blank" rel="noreferrer noopener">Visit the full program</a> to begin adding Technical Papers sessions to your schedule, and be sure to attend the <a href="https://s2023.siggraph.org/full-program/?filter1=sstype128" target="_blank" rel="noreferrer noopener">Papers Fast Forward</a> on Sunday, 6 August at 6 pm PDT.</em></p>

</div><div class="et_pb_module et_pb_post_title et_pb_post_title_1_tb_body post-meta-tags et_pb_bg_layout_light  et_pb_text_align_left">





<div class="et_pb_title_container">

<p class="et_pb_title_meta_container"><span class="dbdb_posttitle_tags_separator">   </span><span class="dbdb_posttitle_tags"><a href="https://blog.siggraph.org/tag/alla-sheffer/" rel="tag">Alla Sheffer</a> <a href="https://blog.siggraph.org/tag/research/" rel="tag">Research</a> <a href="https://blog.siggraph.org/tag/siggraph-2023/" rel="tag">SIGGRAPH 2023</a> <a href="https://blog.siggraph.org/tag/technical-papers/" rel="tag">Technical Papers</a> <a href="https://blog.siggraph.org/tag/technical-papers-awards/" rel="tag">Technical Papers awards</a></span></p></div>

</div>
</div>




</div>